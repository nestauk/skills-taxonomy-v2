"""
Create training data for sentence classifier.
"""

import spacy
from tqdm import tqdm

import json
import re


def load_raw_data(
    jobs_data_file="inputs/karlis_ojo_manually_labelled/OJO_test_labelling_April2021_jobs.jsonl",
):

    with open(jobs_data_file, "r") as file:
        jobs_data = [json.loads(line) for line in file]

    return jobs_data


def mask_text(
    nlp,
    text,
    spacy_ner_types=["DATE", "MONEY", "CARDINAL", "TIME", "ORDINAL", "QUANTITY"],
):
    # Cleaning where you need to keep indices the same
    doc = nlp(text)
    # '6 months' -> '######'
    # Go through detected entities backwards and replace in the sentence
    # (if you did it it going forwards the indexes would be incorrect)
    # Need to mask with the same length of entity so indexes of skills are still valid
    for entity in reversed(doc.ents):
        if entity.label_ in spacy_ner_types:
            text = (
                text[: entity.start_char]
                + "#" * len(str(entity))
                + text[entity.end_char :]
            )

    # Replace any * with comma
    text = text.replace("*", ",")
    text = text.replace("â€¢", ",")
    text = text.replace("-", ",")
    return text


def text_cleaning(text):
    # Cleaning where it doesnt matter if you mess up the indices
    text = re.sub(r"[#]+", "NUMBER", text)
    text = re.sub(
        "CNUMBER", "C#", text
    )  # Some situations you shouldn't have removed the numbers
    return text


def split_labelled_sentences(nlp, text, skills_annotations, skill_label_ids=[1, 5]):
    """
    Tag the sentences within one job advert text as having skills in (1) or not (0).
    - Get job advert text and corresponding annotations of skills within it
    - Clean text
    - Split up sentences and check if they contained any skill labels
    - Create list of sentences with corresponding list of tags
    """

    skill_spans = [
        (label["start_offset"], label["end_offset"])
        for label in skills_annotations
        if label["label"] in skill_label_ids
    ]

    # Mask out numbers (good to do before sentence splitting since they can look like sentence boundaries) and remove *, but doesn't effect number of characters
    text = mask_text(nlp, text)

    # Split up sentences
    skill_span_sets = [set(range(s, e)) for s, e in skill_spans]
    doc = nlp(text)
    sentences = []
    sentences_label = []
    for sent in doc.sents:
        sentences.append(text_cleaning(sent.text))
        sentence_set = set(range(sent.start_char, sent.end_char))
        if any([entity_set.issubset(sentence_set) for entity_set in skill_span_sets]):
            sentences_label.append(1)
        else:
            sentences_label.append(0)

    return sentences, sentences_label


def create_training_data(
    nlp, jobs_data, sentence_train_threshold=15, skill_label_ids=[1, 5]
):
    """
    Label all job adverts and add them all to a list of training data.
    sentence_train_threshold: A threshold of how big the sentence has to be in order to include it in the training/test data
    """

    # Create training dataset
    training_data = []
    for job_info in tqdm(jobs_data):
        text = job_info["text"]
        skills_annotations = job_info["annotations"]
        sentences, sentences_label = split_labelled_sentences(
            nlp, text, skills_annotations, skill_label_ids=skill_label_ids
        )
        for sentence, sentence_label in zip(sentences, sentences_label):
            if len(sentence) > sentence_train_threshold:
                training_data.append((sentence, sentence_label))

    return training_data


def save_training_data(training_data, output_file):
    with open(output_file, "w") as file:
        for label in training_data:
            file.write(json.dumps(label))
            file.write("\n")


def load_training_data(input_file):
    with open(input_file, "r") as file:
        training_data = [tuple(json.loads(line)) for line in file]
    return training_data


if __name__ == "__main__":

    jobs_data = load_raw_data(
        jobs_data_file="inputs/karlis_ojo_manually_labelled/OJO_test_labelling_April2021_jobs.jsonl"
    )

    nlp = spacy.load("en_core_web_sm")

    training_data = create_training_data(nlp, jobs_data, sentence_train_threshold=15)

    save_training_data(
        training_data, "outputs/sentence_classifier/data/training_data_April2021.json"
    )
