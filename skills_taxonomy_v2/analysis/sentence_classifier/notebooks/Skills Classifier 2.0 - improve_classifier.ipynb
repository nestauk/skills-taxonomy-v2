{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from itertools import zip_longest\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c6cbc",
   "metadata": {},
   "source": [
    "## import liz's pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef392a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skills_taxonomy_v2.pipeline.sentence_classifier.create_training_data import *\n",
    "from skills_taxonomy_v2.pipeline.sentence_classifier.predict_sentence_class import *\n",
    "from skills_taxonomy_v2.pipeline.sentence_classifier.sentence_classifier import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfdb0a",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29fd64be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9237"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#both karlis and label-studio\n",
    "\n",
    "with open('/Users/india.kerlenesta/Projects/skills-taxonomy-v2/inputs/new_training_data/training_data.pickle', 'rb') as fp:\n",
    "    training_data = pickle.load(fp)\n",
    "\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78adaba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just label-studio\n",
    "\n",
    "with open('/inputs/new_training_data/label_studio_training_data.pickle', 'rb') as fp:\n",
    "    label_training_data = pickle.load(fp)\n",
    "\n",
    "len(label_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c743223",
   "metadata": {},
   "source": [
    "## preprocess sentence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d923b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers, symbols, lowercase, strip trailing white space\n",
    "\n",
    "def preprocess_training_data(training):\n",
    "    return [(re.sub(r'\\d+[^\\w]', ' ', string[0]).lower().strip(), string[1]) for string in training]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4f571",
   "metadata": {},
   "source": [
    "## Experiment No. 1 - test liz's pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb1b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 8313\n",
      "Size of test data: 924\n",
      "Counter of training data classes: Counter({0: 6573, 1: 1740})\n",
      "Counter of test data classes: Counter({0: 731, 1: 193})\n",
      "2021-08-05 15:21:45,531 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:21:45,532 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:21:45,533 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 15:21:45,534 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:21:45,831 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 8313 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:21:45,831 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:21:45,832 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:21:52,516 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 208\n",
      "Took 72.39403891563416 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88      6573\n",
      "           1       0.56      0.86      0.68      1740\n",
      "\n",
      "    accuracy                           0.83      8313\n",
      "   macro avg       0.76      0.84      0.78      8313\n",
      "weighted avg       0.87      0.83      0.84      8313\n",
      "\n",
      "[[5376 1197]\n",
      " [ 242 1498]]\n",
      "Getting embeddings for 924 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:22:58,963 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:22:58,964 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:23:05,570 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 24\n",
      "Took 19.171248197555542 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.88       731\n",
      "           1       0.54      0.82      0.65       193\n",
      "\n",
      "    accuracy                           0.82       924\n",
      "   macro avg       0.74      0.82      0.76       924\n",
      "weighted avg       0.86      0.82      0.83       924\n",
      "\n",
      "[[596 135]\n",
      " [ 35 158]]\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "\n",
    "sent_class = SentenceClassifier(\n",
    "    split_random_seed=1,\n",
    "    test_size=0.1,\n",
    "    log_reg_max_iter=1000,\n",
    "    bert_model_name='paraphrase-MiniLM-L6-v2',\n",
    "    multi_process=True,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = sent_class.split_data(\n",
    "    training_data, verbose=True\n",
    ")\n",
    "\n",
    "X_train_vec = sent_class.fit_transform(X_train)\n",
    "sent_class.fit(X_train_vec, y_train)\n",
    "\n",
    "# Training evaluation\n",
    "y_train_pred = sent_class.predict(X_train_vec)\n",
    "class_rep_train = sent_class.evaluate(y_train, y_train_pred, verbose=True)\n",
    "\n",
    "# Test evaluation\n",
    "y_test_pred = sent_class.predict_transform(X_test)\n",
    "class_rep_test = sent_class.evaluate(y_test, y_test_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686717ce",
   "metadata": {},
   "source": [
    "## Experiment No. 2 - test liz's pipeline on label studio data alone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861cf133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 7390\n",
      "Size of test data: 822\n",
      "Counter of training data classes: Counter({0: 6089, 1: 1301})\n",
      "Counter of test data classes: Counter({0: 677, 1: 145})\n",
      "2021-08-05 15:24:03,697 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:24:03,697 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:24:03,698 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 15:24:03,699 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:24:03,994 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 7390 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:24:03,994 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:24:03,995 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:24:10,741 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 185\n",
      "Took 65.95750498771667 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89      6089\n",
      "           1       0.52      0.88      0.66      1301\n",
      "\n",
      "    accuracy                           0.84      7390\n",
      "   macro avg       0.75      0.85      0.78      7390\n",
      "weighted avg       0.89      0.84      0.85      7390\n",
      "\n",
      "[[5053 1036]\n",
      " [ 157 1144]]\n",
      "Getting embeddings for 822 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:25:10,630 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:25:10,630 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:25:17,253 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 21\n",
      "Took 16.723246097564697 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.81      0.88       677\n",
      "           1       0.49      0.87      0.63       145\n",
      "\n",
      "    accuracy                           0.82       822\n",
      "   macro avg       0.73      0.84      0.75       822\n",
      "weighted avg       0.88      0.82      0.83       822\n",
      "\n",
      "[[545 132]\n",
      " [ 19 126]]\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "\n",
    "sent_class = SentenceClassifier(\n",
    "    split_random_seed=1,\n",
    "    test_size=0.1,\n",
    "    log_reg_max_iter=1000,\n",
    "    bert_model_name='paraphrase-MiniLM-L6-v2',\n",
    "    multi_process=True,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = sent_class.split_data(\n",
    "    label_training_data, verbose=True\n",
    ")\n",
    "\n",
    "X_train_vec = sent_class.fit_transform(X_train)\n",
    "sent_class.fit(X_train_vec, y_train)\n",
    "\n",
    "# Training evaluation\n",
    "y_train_pred = sent_class.predict(X_train_vec)\n",
    "class_rep_train = sent_class.evaluate(y_train, y_train_pred, verbose=True)\n",
    "\n",
    "# Test evaluation\n",
    "y_test_pred = sent_class.predict_transform(X_test)\n",
    "class_rep_test = sent_class.evaluate(y_test, y_test_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab684ac",
   "metadata": {},
   "source": [
    "## Experiment No. 3 - preprocess text + all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d029b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 8313\n",
      "Size of test data: 924\n",
      "Counter of training data classes: Counter({0: 6573, 1: 1740})\n",
      "Counter of test data classes: Counter({0: 731, 1: 193})\n",
      "2021-08-06 09:50:17,069 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:50:17,069 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:50:17,070 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-06 09:50:17,071 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:50:17,395 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 8313 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-06 09:50:17,396 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-06 09:50:17,396 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-06 09:50:24,751 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 208\n",
      "Took 73.31519198417664 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88      6573\n",
      "           1       0.55      0.86      0.67      1740\n",
      "\n",
      "    accuracy                           0.83      8313\n",
      "   macro avg       0.76      0.84      0.78      8313\n",
      "weighted avg       0.87      0.83      0.84      8313\n",
      "\n",
      "[[5368 1205]\n",
      " [ 240 1500]]\n",
      "Getting embeddings for 924 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-06 09:51:31,323 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-06 09:51:31,324 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-06 09:51:38,372 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 24\n",
      "Took 18.755445957183838 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       731\n",
      "           1       0.54      0.82      0.65       193\n",
      "\n",
      "    accuracy                           0.82       924\n",
      "   macro avg       0.74      0.82      0.77       924\n",
      "weighted avg       0.86      0.82      0.83       924\n",
      "\n",
      "[[597 134]\n",
      " [ 34 159]]\n"
     ]
    }
   ],
   "source": [
    "sent_class = SentenceClassifier(\n",
    "    split_random_seed=1,\n",
    "    test_size=0.1,\n",
    "    log_reg_max_iter=1000,\n",
    "    bert_model_name='paraphrase-MiniLM-L6-v2',\n",
    "    multi_process=True,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = sent_class.split_data(\n",
    "    clean_training, verbose=True\n",
    ")\n",
    "\n",
    "X_train_vec = sent_class.fit_transform(X_train)\n",
    "sent_class.fit(X_train_vec, y_train)\n",
    "\n",
    "# Training evaluation\n",
    "y_train_pred = sent_class.predict(X_train_vec)\n",
    "class_rep_train = sent_class.evaluate(y_train, y_train_pred, verbose=True)\n",
    "\n",
    "# Test evaluation\n",
    "y_test_pred = sent_class.predict_transform(X_test)\n",
    "class_rep_test = sent_class.evaluate(y_test, y_test_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1368d",
   "metadata": {},
   "source": [
    "## Experiment No. 4 - use XGBOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bed6955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-05 15:28:17,232 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:28:17,234 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:28:17,235 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 15:28:17,237 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:28:17,557 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 924 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:28:17,557 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:28:17,558 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:28:24,165 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 24\n",
      "Took 18.03501296043396 seconds\n",
      "[15:28:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       731\n",
      "           1       0.73      0.49      0.58       193\n",
      "\n",
      "    accuracy                           0.85       924\n",
      "   macro avg       0.80      0.72      0.75       924\n",
      "weighted avg       0.84      0.85      0.84       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#vectorise test data \n",
    "X_test_vec = sent_class.fit_transform(X_test)\n",
    "\n",
    "# run on xgboost\n",
    "\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(X_train_vec, y_train)\n",
    "predict = xgb.predict(X_test_vec)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e92e1d",
   "metadata": {},
   "source": [
    "## Experiment No. 5 - Balance training data - undersample 0 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cfd1210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced training sentences is now Counter({0: 1740, 1: 1740})\n",
      "test set is Counter({0: 731, 1: 193})\n"
     ]
    }
   ],
   "source": [
    "# get all skill labels in training set from clean_training\n",
    "skills = [(train, label) for train, label in zip(X_train, y_train) if label == 1]\n",
    "\n",
    "# randomly sample non skill sentences \n",
    "no_skill_undersample = random.sample([(train, label) for train, label in zip(X_train, y_train) if label == 0], len(skills))\n",
    "\n",
    "# create new balanced training set \n",
    "balanced_training = no_skill_undersample + skills\n",
    "\n",
    "X_train_undersample = [x[0] for x in balanced_training]\n",
    "y_train_undersample = [x[1] for x in balanced_training]\n",
    "\n",
    "print(f'balanced training sentences is now {Counter(y_train_undersample)}')\n",
    "print(f'test set is {Counter(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6106440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-05 15:32:27,902 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:32:27,904 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:32:27,905 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 15:32:27,906 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:32:28,236 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 3480 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:32:28,236 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:32:28,236 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:32:34,880 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 87\n",
      "Took 40.610546827316284 seconds\n"
     ]
    }
   ],
   "source": [
    "# vectorise training data\n",
    "X_train_vec = sent_class.fit_transform(X_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4be64afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       731\n",
      "           1       0.52      0.85      0.64       193\n",
      "\n",
      "    accuracy                           0.80       924\n",
      "   macro avg       0.73      0.82      0.75       924\n",
      "weighted avg       0.86      0.80      0.82       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run balanced data on xgboost\n",
    "\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(X_train_vec, y_train_undersample)\n",
    "predict = xgb.predict(X_test_vec)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f3d0d",
   "metadata": {},
   "source": [
    "## Experiment No. 6 - Balance training data - use nlpaug word synonyms to oversample 1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8496e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate augmented skills sentences with wordnet + balance classes\n",
    "# oversample 1 class\n",
    "\n",
    "def oversample_skills_wordnet(skills_data):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_skills = [(aug.augment(train[0]), train[1]) for train in skills_data]\n",
    "    return skills_data + augmented_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c03ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced training sentences is now Counter({0: 6573, 1: 3480})\n",
      "10053\n",
      "test set is Counter({0: 731, 1: 193})\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "#make training data\n",
    "skills_augment = oversample_skills_wordnet(skills)\n",
    "balanced_augment_training = skills_augment + [(train, label) for train, label in zip(X_train, y_train) if label == 0]\n",
    "\n",
    "X_train_oversample_syns = [x[0] for x in balanced_augment_training]\n",
    "y_train_oversample_syns = [x[1] for x in balanced_augment_training]\n",
    "\n",
    "print(f'balanced training sentences is now {Counter(y_train_oversample_syns)}')\n",
    "print(len(y_train_oversample_syns))\n",
    "print(f'test set is {Counter(y_test)}')\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f0d009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-05 15:34:58,841 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:34:58,843 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:34:58,845 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 15:34:58,848 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 15:34:59,161 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 10053 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 15:34:59,161 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 15:34:59,162 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 15:35:06,001 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 252\n",
      "Took 82.13852095603943 seconds\n"
     ]
    }
   ],
   "source": [
    "# vectorise new training\n",
    "X_train_vec = sent_class.fit_transform(X_train_oversample_syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6154372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       731\n",
      "           1       0.69      0.68      0.68       193\n",
      "\n",
      "    accuracy                           0.87       924\n",
      "   macro avg       0.80      0.80      0.80       924\n",
      "weighted avg       0.87      0.87      0.87       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run balanced data on xgboost\n",
    "\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(X_train_vec, y_train_oversample_syns)\n",
    "predict = xgb.predict(X_test_vec)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec43da",
   "metadata": {},
   "source": [
    "## Experiment No. 7 - Balance training data - use contextual word embeddings to oversample 1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8a5c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate augmented skills sentences with wordnet + balance classes\n",
    "\n",
    "def oversample_skills_embeds(skills_data):\n",
    "    stops = stopwords.words('english')\n",
    "    aug = naw.ContextualWordEmbsAug(aug_min = 1, stopwords = stops)\n",
    "    augmented_embed_skill_sents = []\n",
    "    for index, train in enumerate(skills):\n",
    "        augment_word_embeds = aug.augment(train[0])\n",
    "        print(f'augmented {index} sentence!')\n",
    "        augmented_embed_skill_sents.append((augment_word_embeds, train[1]))\n",
    "    return skills_data + augmented_embed_skill_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51e11fa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented 0 sentence!\n",
      "augmented 1 sentence!\n",
      "augmented 2 sentence!\n",
      "augmented 3 sentence!\n",
      "augmented 4 sentence!\n",
      "augmented 5 sentence!\n",
      "augmented 6 sentence!\n",
      "augmented 7 sentence!\n",
      "augmented 8 sentence!\n",
      "augmented 9 sentence!\n",
      "augmented 10 sentence!\n",
      "augmented 11 sentence!\n",
      "augmented 12 sentence!\n",
      "augmented 13 sentence!\n",
      "augmented 14 sentence!\n",
      "augmented 15 sentence!\n",
      "augmented 16 sentence!\n",
      "augmented 17 sentence!\n",
      "augmented 18 sentence!\n",
      "augmented 19 sentence!\n",
      "augmented 20 sentence!\n",
      "augmented 21 sentence!\n",
      "augmented 22 sentence!\n",
      "augmented 23 sentence!\n",
      "augmented 24 sentence!\n",
      "augmented 25 sentence!\n",
      "augmented 26 sentence!\n",
      "augmented 27 sentence!\n",
      "augmented 28 sentence!\n",
      "augmented 29 sentence!\n",
      "augmented 30 sentence!\n",
      "augmented 31 sentence!\n",
      "augmented 32 sentence!\n",
      "augmented 33 sentence!\n",
      "augmented 34 sentence!\n",
      "augmented 35 sentence!\n",
      "augmented 36 sentence!\n",
      "augmented 37 sentence!\n",
      "augmented 38 sentence!\n",
      "augmented 39 sentence!\n",
      "augmented 40 sentence!\n",
      "augmented 41 sentence!\n",
      "augmented 42 sentence!\n",
      "augmented 43 sentence!\n",
      "augmented 44 sentence!\n",
      "augmented 45 sentence!\n",
      "augmented 46 sentence!\n",
      "augmented 47 sentence!\n",
      "augmented 48 sentence!\n",
      "augmented 49 sentence!\n",
      "augmented 50 sentence!\n",
      "augmented 51 sentence!\n",
      "augmented 52 sentence!\n",
      "augmented 53 sentence!\n",
      "augmented 54 sentence!\n",
      "augmented 55 sentence!\n",
      "augmented 56 sentence!\n",
      "augmented 57 sentence!\n",
      "augmented 58 sentence!\n",
      "augmented 59 sentence!\n",
      "augmented 60 sentence!\n",
      "augmented 61 sentence!\n",
      "augmented 62 sentence!\n",
      "augmented 63 sentence!\n",
      "augmented 64 sentence!\n",
      "augmented 65 sentence!\n",
      "augmented 66 sentence!\n",
      "augmented 67 sentence!\n",
      "augmented 68 sentence!\n",
      "augmented 69 sentence!\n",
      "augmented 70 sentence!\n",
      "augmented 71 sentence!\n",
      "augmented 72 sentence!\n",
      "augmented 73 sentence!\n",
      "augmented 74 sentence!\n",
      "augmented 75 sentence!\n",
      "augmented 76 sentence!\n",
      "augmented 77 sentence!\n",
      "augmented 78 sentence!\n",
      "augmented 79 sentence!\n",
      "augmented 80 sentence!\n",
      "augmented 81 sentence!\n",
      "augmented 82 sentence!\n",
      "augmented 83 sentence!\n",
      "augmented 84 sentence!\n",
      "augmented 85 sentence!\n",
      "augmented 86 sentence!\n",
      "augmented 87 sentence!\n",
      "augmented 88 sentence!\n",
      "augmented 89 sentence!\n",
      "augmented 90 sentence!\n",
      "augmented 91 sentence!\n",
      "augmented 92 sentence!\n",
      "augmented 93 sentence!\n",
      "augmented 94 sentence!\n",
      "augmented 95 sentence!\n",
      "augmented 96 sentence!\n",
      "augmented 97 sentence!\n",
      "augmented 98 sentence!\n",
      "augmented 99 sentence!\n",
      "augmented 100 sentence!\n",
      "augmented 101 sentence!\n",
      "augmented 102 sentence!\n",
      "augmented 103 sentence!\n",
      "augmented 104 sentence!\n",
      "augmented 105 sentence!\n",
      "augmented 106 sentence!\n",
      "augmented 107 sentence!\n",
      "augmented 108 sentence!\n",
      "augmented 109 sentence!\n",
      "augmented 110 sentence!\n",
      "augmented 111 sentence!\n",
      "augmented 112 sentence!\n",
      "augmented 113 sentence!\n",
      "augmented 114 sentence!\n",
      "augmented 115 sentence!\n",
      "augmented 116 sentence!\n",
      "augmented 117 sentence!\n",
      "augmented 118 sentence!\n",
      "augmented 119 sentence!\n",
      "augmented 120 sentence!\n",
      "augmented 121 sentence!\n",
      "augmented 122 sentence!\n",
      "augmented 123 sentence!\n",
      "augmented 124 sentence!\n",
      "augmented 125 sentence!\n",
      "augmented 126 sentence!\n",
      "augmented 127 sentence!\n",
      "augmented 128 sentence!\n",
      "augmented 129 sentence!\n",
      "augmented 130 sentence!\n",
      "augmented 131 sentence!\n",
      "augmented 132 sentence!\n",
      "augmented 133 sentence!\n",
      "augmented 134 sentence!\n",
      "augmented 135 sentence!\n",
      "augmented 136 sentence!\n",
      "augmented 137 sentence!\n",
      "augmented 138 sentence!\n",
      "augmented 139 sentence!\n",
      "augmented 140 sentence!\n",
      "augmented 141 sentence!\n",
      "augmented 142 sentence!\n",
      "augmented 143 sentence!\n",
      "augmented 144 sentence!\n",
      "augmented 145 sentence!\n",
      "augmented 146 sentence!\n",
      "augmented 147 sentence!\n",
      "augmented 148 sentence!\n",
      "augmented 149 sentence!\n",
      "augmented 150 sentence!\n",
      "augmented 151 sentence!\n",
      "augmented 152 sentence!\n",
      "augmented 153 sentence!\n",
      "augmented 154 sentence!\n",
      "augmented 155 sentence!\n",
      "augmented 156 sentence!\n",
      "augmented 157 sentence!\n",
      "augmented 158 sentence!\n",
      "augmented 159 sentence!\n",
      "augmented 160 sentence!\n",
      "augmented 161 sentence!\n",
      "augmented 162 sentence!\n",
      "augmented 163 sentence!\n",
      "augmented 164 sentence!\n",
      "augmented 165 sentence!\n",
      "augmented 166 sentence!\n",
      "augmented 167 sentence!\n",
      "augmented 168 sentence!\n",
      "augmented 169 sentence!\n",
      "augmented 170 sentence!\n",
      "augmented 171 sentence!\n",
      "augmented 172 sentence!\n",
      "augmented 173 sentence!\n",
      "augmented 174 sentence!\n",
      "augmented 175 sentence!\n",
      "augmented 176 sentence!\n",
      "augmented 177 sentence!\n",
      "augmented 178 sentence!\n",
      "augmented 179 sentence!\n",
      "augmented 180 sentence!\n",
      "augmented 181 sentence!\n",
      "augmented 182 sentence!\n",
      "augmented 183 sentence!\n",
      "augmented 184 sentence!\n",
      "augmented 185 sentence!\n",
      "augmented 186 sentence!\n",
      "augmented 187 sentence!\n",
      "augmented 188 sentence!\n",
      "augmented 189 sentence!\n",
      "augmented 190 sentence!\n",
      "augmented 191 sentence!\n",
      "augmented 192 sentence!\n",
      "augmented 193 sentence!\n",
      "augmented 194 sentence!\n",
      "augmented 195 sentence!\n",
      "augmented 196 sentence!\n",
      "augmented 197 sentence!\n",
      "augmented 198 sentence!\n",
      "augmented 199 sentence!\n",
      "augmented 200 sentence!\n",
      "augmented 201 sentence!\n",
      "augmented 202 sentence!\n",
      "augmented 203 sentence!\n",
      "augmented 204 sentence!\n",
      "augmented 205 sentence!\n",
      "augmented 206 sentence!\n",
      "augmented 207 sentence!\n",
      "augmented 208 sentence!\n",
      "augmented 209 sentence!\n",
      "augmented 210 sentence!\n",
      "augmented 211 sentence!\n",
      "augmented 212 sentence!\n",
      "augmented 213 sentence!\n",
      "augmented 214 sentence!\n",
      "augmented 215 sentence!\n",
      "augmented 216 sentence!\n",
      "augmented 217 sentence!\n",
      "augmented 218 sentence!\n",
      "augmented 219 sentence!\n",
      "augmented 220 sentence!\n",
      "augmented 221 sentence!\n",
      "augmented 222 sentence!\n",
      "augmented 223 sentence!\n",
      "augmented 224 sentence!\n",
      "augmented 225 sentence!\n",
      "augmented 226 sentence!\n",
      "augmented 227 sentence!\n",
      "augmented 228 sentence!\n",
      "augmented 229 sentence!\n",
      "augmented 230 sentence!\n",
      "augmented 231 sentence!\n",
      "augmented 232 sentence!\n",
      "augmented 233 sentence!\n",
      "augmented 234 sentence!\n",
      "augmented 235 sentence!\n",
      "augmented 236 sentence!\n",
      "augmented 237 sentence!\n",
      "augmented 238 sentence!\n",
      "augmented 239 sentence!\n",
      "augmented 240 sentence!\n",
      "augmented 241 sentence!\n",
      "augmented 242 sentence!\n",
      "augmented 243 sentence!\n",
      "augmented 244 sentence!\n",
      "augmented 245 sentence!\n",
      "augmented 246 sentence!\n",
      "augmented 247 sentence!\n",
      "augmented 248 sentence!\n",
      "augmented 249 sentence!\n",
      "augmented 250 sentence!\n",
      "augmented 251 sentence!\n",
      "augmented 252 sentence!\n",
      "augmented 253 sentence!\n",
      "augmented 254 sentence!\n",
      "augmented 255 sentence!\n",
      "augmented 256 sentence!\n",
      "augmented 257 sentence!\n",
      "augmented 258 sentence!\n",
      "augmented 259 sentence!\n",
      "augmented 260 sentence!\n",
      "augmented 261 sentence!\n",
      "augmented 262 sentence!\n",
      "augmented 263 sentence!\n",
      "augmented 264 sentence!\n",
      "augmented 265 sentence!\n",
      "augmented 266 sentence!\n",
      "augmented 267 sentence!\n",
      "augmented 268 sentence!\n",
      "augmented 269 sentence!\n",
      "augmented 270 sentence!\n",
      "augmented 271 sentence!\n",
      "augmented 272 sentence!\n",
      "augmented 273 sentence!\n",
      "augmented 274 sentence!\n",
      "augmented 275 sentence!\n",
      "augmented 276 sentence!\n",
      "augmented 277 sentence!\n",
      "augmented 278 sentence!\n",
      "augmented 279 sentence!\n",
      "augmented 280 sentence!\n",
      "augmented 281 sentence!\n",
      "augmented 282 sentence!\n",
      "augmented 283 sentence!\n",
      "augmented 284 sentence!\n",
      "augmented 285 sentence!\n",
      "augmented 286 sentence!\n",
      "augmented 287 sentence!\n",
      "augmented 288 sentence!\n",
      "augmented 289 sentence!\n",
      "augmented 290 sentence!\n",
      "augmented 291 sentence!\n",
      "augmented 292 sentence!\n",
      "augmented 293 sentence!\n",
      "augmented 294 sentence!\n",
      "augmented 295 sentence!\n",
      "augmented 296 sentence!\n",
      "augmented 297 sentence!\n",
      "augmented 298 sentence!\n",
      "augmented 299 sentence!\n",
      "augmented 300 sentence!\n",
      "augmented 301 sentence!\n",
      "augmented 302 sentence!\n",
      "augmented 303 sentence!\n",
      "augmented 304 sentence!\n",
      "augmented 305 sentence!\n",
      "augmented 306 sentence!\n",
      "augmented 307 sentence!\n",
      "augmented 308 sentence!\n",
      "augmented 309 sentence!\n",
      "augmented 310 sentence!\n",
      "augmented 311 sentence!\n",
      "augmented 312 sentence!\n",
      "augmented 313 sentence!\n",
      "augmented 314 sentence!\n",
      "augmented 315 sentence!\n",
      "augmented 316 sentence!\n",
      "augmented 317 sentence!\n",
      "augmented 318 sentence!\n",
      "augmented 319 sentence!\n",
      "augmented 320 sentence!\n",
      "augmented 321 sentence!\n",
      "augmented 322 sentence!\n",
      "augmented 323 sentence!\n",
      "augmented 324 sentence!\n",
      "augmented 325 sentence!\n",
      "augmented 326 sentence!\n",
      "augmented 327 sentence!\n",
      "augmented 328 sentence!\n",
      "augmented 329 sentence!\n",
      "augmented 330 sentence!\n",
      "augmented 331 sentence!\n",
      "augmented 332 sentence!\n",
      "augmented 333 sentence!\n",
      "augmented 334 sentence!\n",
      "augmented 335 sentence!\n",
      "augmented 336 sentence!\n",
      "augmented 337 sentence!\n",
      "augmented 338 sentence!\n",
      "augmented 339 sentence!\n",
      "augmented 340 sentence!\n",
      "augmented 341 sentence!\n",
      "augmented 342 sentence!\n",
      "augmented 343 sentence!\n",
      "augmented 344 sentence!\n",
      "augmented 345 sentence!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented 346 sentence!\n",
      "augmented 347 sentence!\n",
      "augmented 348 sentence!\n",
      "augmented 349 sentence!\n",
      "augmented 350 sentence!\n",
      "augmented 351 sentence!\n",
      "augmented 352 sentence!\n",
      "augmented 353 sentence!\n",
      "augmented 354 sentence!\n",
      "augmented 355 sentence!\n",
      "augmented 356 sentence!\n",
      "augmented 357 sentence!\n",
      "augmented 358 sentence!\n",
      "augmented 359 sentence!\n",
      "augmented 360 sentence!\n",
      "augmented 361 sentence!\n",
      "augmented 362 sentence!\n",
      "augmented 363 sentence!\n",
      "augmented 364 sentence!\n",
      "augmented 365 sentence!\n",
      "augmented 366 sentence!\n",
      "augmented 367 sentence!\n",
      "augmented 368 sentence!\n",
      "augmented 369 sentence!\n",
      "augmented 370 sentence!\n",
      "augmented 371 sentence!\n",
      "augmented 372 sentence!\n",
      "augmented 373 sentence!\n",
      "augmented 374 sentence!\n",
      "augmented 375 sentence!\n",
      "augmented 376 sentence!\n",
      "augmented 377 sentence!\n",
      "augmented 378 sentence!\n",
      "augmented 379 sentence!\n",
      "augmented 380 sentence!\n",
      "augmented 381 sentence!\n",
      "augmented 382 sentence!\n",
      "augmented 383 sentence!\n",
      "augmented 384 sentence!\n",
      "augmented 385 sentence!\n",
      "augmented 386 sentence!\n",
      "augmented 387 sentence!\n",
      "augmented 388 sentence!\n",
      "augmented 389 sentence!\n",
      "augmented 390 sentence!\n",
      "augmented 391 sentence!\n",
      "augmented 392 sentence!\n",
      "augmented 393 sentence!\n",
      "augmented 394 sentence!\n",
      "augmented 395 sentence!\n",
      "augmented 396 sentence!\n",
      "augmented 397 sentence!\n",
      "augmented 398 sentence!\n",
      "augmented 399 sentence!\n",
      "augmented 400 sentence!\n",
      "augmented 401 sentence!\n",
      "augmented 402 sentence!\n",
      "augmented 403 sentence!\n",
      "augmented 404 sentence!\n",
      "augmented 405 sentence!\n",
      "augmented 406 sentence!\n",
      "augmented 407 sentence!\n",
      "augmented 408 sentence!\n",
      "augmented 409 sentence!\n",
      "augmented 410 sentence!\n",
      "augmented 411 sentence!\n",
      "augmented 412 sentence!\n",
      "augmented 413 sentence!\n",
      "augmented 414 sentence!\n",
      "augmented 415 sentence!\n",
      "augmented 416 sentence!\n",
      "augmented 417 sentence!\n",
      "augmented 418 sentence!\n",
      "augmented 419 sentence!\n",
      "augmented 420 sentence!\n",
      "augmented 421 sentence!\n",
      "augmented 422 sentence!\n",
      "augmented 423 sentence!\n",
      "augmented 424 sentence!\n",
      "augmented 425 sentence!\n",
      "augmented 426 sentence!\n",
      "augmented 427 sentence!\n",
      "augmented 428 sentence!\n",
      "augmented 429 sentence!\n",
      "augmented 430 sentence!\n",
      "augmented 431 sentence!\n",
      "augmented 432 sentence!\n",
      "augmented 433 sentence!\n",
      "augmented 434 sentence!\n",
      "augmented 435 sentence!\n",
      "augmented 436 sentence!\n",
      "augmented 437 sentence!\n",
      "augmented 438 sentence!\n",
      "augmented 439 sentence!\n",
      "augmented 440 sentence!\n",
      "augmented 441 sentence!\n",
      "augmented 442 sentence!\n",
      "augmented 443 sentence!\n",
      "augmented 444 sentence!\n",
      "augmented 445 sentence!\n",
      "augmented 446 sentence!\n",
      "augmented 447 sentence!\n",
      "augmented 448 sentence!\n",
      "augmented 449 sentence!\n",
      "augmented 450 sentence!\n",
      "augmented 451 sentence!\n",
      "augmented 452 sentence!\n",
      "augmented 453 sentence!\n",
      "augmented 454 sentence!\n",
      "augmented 455 sentence!\n",
      "augmented 456 sentence!\n",
      "augmented 457 sentence!\n",
      "augmented 458 sentence!\n",
      "augmented 459 sentence!\n",
      "augmented 460 sentence!\n",
      "augmented 461 sentence!\n",
      "augmented 462 sentence!\n",
      "augmented 463 sentence!\n",
      "augmented 464 sentence!\n",
      "augmented 465 sentence!\n",
      "augmented 466 sentence!\n",
      "augmented 467 sentence!\n",
      "augmented 468 sentence!\n",
      "augmented 469 sentence!\n",
      "augmented 470 sentence!\n",
      "augmented 471 sentence!\n",
      "augmented 472 sentence!\n",
      "augmented 473 sentence!\n",
      "augmented 474 sentence!\n",
      "augmented 475 sentence!\n",
      "augmented 476 sentence!\n",
      "augmented 477 sentence!\n",
      "augmented 478 sentence!\n",
      "augmented 479 sentence!\n",
      "augmented 480 sentence!\n",
      "augmented 481 sentence!\n",
      "augmented 482 sentence!\n",
      "augmented 483 sentence!\n",
      "augmented 484 sentence!\n",
      "augmented 485 sentence!\n",
      "augmented 486 sentence!\n",
      "augmented 487 sentence!\n",
      "augmented 488 sentence!\n",
      "augmented 489 sentence!\n",
      "augmented 490 sentence!\n",
      "augmented 491 sentence!\n",
      "augmented 492 sentence!\n",
      "augmented 493 sentence!\n",
      "augmented 494 sentence!\n",
      "augmented 495 sentence!\n",
      "augmented 496 sentence!\n",
      "augmented 497 sentence!\n",
      "augmented 498 sentence!\n",
      "augmented 499 sentence!\n",
      "augmented 500 sentence!\n",
      "augmented 501 sentence!\n",
      "augmented 502 sentence!\n",
      "augmented 503 sentence!\n",
      "augmented 504 sentence!\n",
      "augmented 505 sentence!\n",
      "augmented 506 sentence!\n",
      "augmented 507 sentence!\n",
      "augmented 508 sentence!\n",
      "augmented 509 sentence!\n",
      "augmented 510 sentence!\n",
      "augmented 511 sentence!\n",
      "augmented 512 sentence!\n",
      "augmented 513 sentence!\n",
      "augmented 514 sentence!\n",
      "augmented 515 sentence!\n",
      "augmented 516 sentence!\n",
      "augmented 517 sentence!\n",
      "augmented 518 sentence!\n",
      "augmented 519 sentence!\n",
      "augmented 520 sentence!\n",
      "augmented 521 sentence!\n",
      "augmented 522 sentence!\n",
      "augmented 523 sentence!\n",
      "augmented 524 sentence!\n",
      "augmented 525 sentence!\n",
      "augmented 526 sentence!\n",
      "augmented 527 sentence!\n",
      "augmented 528 sentence!\n",
      "augmented 529 sentence!\n",
      "augmented 530 sentence!\n",
      "augmented 531 sentence!\n",
      "augmented 532 sentence!\n",
      "augmented 533 sentence!\n",
      "augmented 534 sentence!\n",
      "augmented 535 sentence!\n",
      "augmented 536 sentence!\n",
      "augmented 537 sentence!\n",
      "augmented 538 sentence!\n",
      "augmented 539 sentence!\n",
      "augmented 540 sentence!\n",
      "augmented 541 sentence!\n",
      "augmented 542 sentence!\n",
      "augmented 543 sentence!\n",
      "augmented 544 sentence!\n",
      "augmented 545 sentence!\n",
      "augmented 546 sentence!\n",
      "augmented 547 sentence!\n",
      "augmented 548 sentence!\n",
      "augmented 549 sentence!\n",
      "augmented 550 sentence!\n",
      "augmented 551 sentence!\n",
      "augmented 552 sentence!\n",
      "augmented 553 sentence!\n",
      "augmented 554 sentence!\n",
      "augmented 555 sentence!\n",
      "augmented 556 sentence!\n",
      "augmented 557 sentence!\n",
      "augmented 558 sentence!\n",
      "augmented 559 sentence!\n",
      "augmented 560 sentence!\n",
      "augmented 561 sentence!\n",
      "augmented 562 sentence!\n",
      "augmented 563 sentence!\n",
      "augmented 564 sentence!\n",
      "augmented 565 sentence!\n",
      "augmented 566 sentence!\n",
      "augmented 567 sentence!\n",
      "augmented 568 sentence!\n",
      "augmented 569 sentence!\n",
      "augmented 570 sentence!\n",
      "augmented 571 sentence!\n",
      "augmented 572 sentence!\n",
      "augmented 573 sentence!\n",
      "augmented 574 sentence!\n",
      "augmented 575 sentence!\n",
      "augmented 576 sentence!\n",
      "augmented 577 sentence!\n",
      "augmented 578 sentence!\n",
      "augmented 579 sentence!\n",
      "augmented 580 sentence!\n",
      "augmented 581 sentence!\n",
      "augmented 582 sentence!\n",
      "augmented 583 sentence!\n",
      "augmented 584 sentence!\n",
      "augmented 585 sentence!\n",
      "augmented 586 sentence!\n",
      "augmented 587 sentence!\n",
      "augmented 588 sentence!\n",
      "augmented 589 sentence!\n",
      "augmented 590 sentence!\n",
      "augmented 591 sentence!\n",
      "augmented 592 sentence!\n",
      "augmented 593 sentence!\n",
      "augmented 594 sentence!\n",
      "augmented 595 sentence!\n",
      "augmented 596 sentence!\n",
      "augmented 597 sentence!\n",
      "augmented 598 sentence!\n",
      "augmented 599 sentence!\n",
      "augmented 600 sentence!\n",
      "augmented 601 sentence!\n",
      "augmented 602 sentence!\n",
      "augmented 603 sentence!\n",
      "augmented 604 sentence!\n",
      "augmented 605 sentence!\n",
      "augmented 606 sentence!\n",
      "augmented 607 sentence!\n",
      "augmented 608 sentence!\n",
      "augmented 609 sentence!\n",
      "augmented 610 sentence!\n",
      "augmented 611 sentence!\n",
      "augmented 612 sentence!\n",
      "augmented 613 sentence!\n",
      "augmented 614 sentence!\n",
      "augmented 615 sentence!\n",
      "augmented 616 sentence!\n",
      "augmented 617 sentence!\n",
      "augmented 618 sentence!\n",
      "augmented 619 sentence!\n",
      "augmented 620 sentence!\n",
      "augmented 621 sentence!\n",
      "augmented 622 sentence!\n",
      "augmented 623 sentence!\n",
      "augmented 624 sentence!\n",
      "augmented 625 sentence!\n",
      "augmented 626 sentence!\n",
      "augmented 627 sentence!\n",
      "augmented 628 sentence!\n",
      "augmented 629 sentence!\n",
      "augmented 630 sentence!\n",
      "augmented 631 sentence!\n",
      "augmented 632 sentence!\n",
      "augmented 633 sentence!\n",
      "augmented 634 sentence!\n",
      "augmented 635 sentence!\n",
      "augmented 636 sentence!\n",
      "augmented 637 sentence!\n",
      "augmented 638 sentence!\n",
      "augmented 639 sentence!\n",
      "augmented 640 sentence!\n",
      "augmented 641 sentence!\n",
      "augmented 642 sentence!\n",
      "augmented 643 sentence!\n",
      "augmented 644 sentence!\n",
      "augmented 645 sentence!\n",
      "augmented 646 sentence!\n",
      "augmented 647 sentence!\n",
      "augmented 648 sentence!\n",
      "augmented 649 sentence!\n",
      "augmented 650 sentence!\n",
      "augmented 651 sentence!\n",
      "augmented 652 sentence!\n",
      "augmented 653 sentence!\n",
      "augmented 654 sentence!\n",
      "augmented 655 sentence!\n",
      "augmented 656 sentence!\n",
      "augmented 657 sentence!\n",
      "augmented 658 sentence!\n",
      "augmented 659 sentence!\n",
      "augmented 660 sentence!\n",
      "augmented 661 sentence!\n",
      "augmented 662 sentence!\n",
      "augmented 663 sentence!\n",
      "augmented 664 sentence!\n",
      "augmented 665 sentence!\n",
      "augmented 666 sentence!\n",
      "augmented 667 sentence!\n",
      "augmented 668 sentence!\n",
      "augmented 669 sentence!\n",
      "augmented 670 sentence!\n",
      "augmented 671 sentence!\n",
      "augmented 672 sentence!\n",
      "augmented 673 sentence!\n",
      "augmented 674 sentence!\n",
      "augmented 675 sentence!\n",
      "augmented 676 sentence!\n",
      "augmented 677 sentence!\n",
      "augmented 678 sentence!\n",
      "augmented 679 sentence!\n",
      "augmented 680 sentence!\n",
      "augmented 681 sentence!\n",
      "augmented 682 sentence!\n",
      "augmented 683 sentence!\n",
      "augmented 684 sentence!\n",
      "augmented 685 sentence!\n",
      "augmented 686 sentence!\n",
      "augmented 687 sentence!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented 688 sentence!\n",
      "augmented 689 sentence!\n",
      "augmented 690 sentence!\n",
      "augmented 691 sentence!\n",
      "augmented 692 sentence!\n",
      "augmented 693 sentence!\n",
      "augmented 694 sentence!\n",
      "augmented 695 sentence!\n",
      "augmented 696 sentence!\n",
      "augmented 697 sentence!\n",
      "augmented 698 sentence!\n",
      "augmented 699 sentence!\n",
      "augmented 700 sentence!\n",
      "augmented 701 sentence!\n",
      "augmented 702 sentence!\n",
      "augmented 703 sentence!\n",
      "augmented 704 sentence!\n",
      "augmented 705 sentence!\n",
      "augmented 706 sentence!\n",
      "augmented 707 sentence!\n",
      "augmented 708 sentence!\n",
      "augmented 709 sentence!\n",
      "augmented 710 sentence!\n",
      "augmented 711 sentence!\n",
      "augmented 712 sentence!\n",
      "augmented 713 sentence!\n",
      "augmented 714 sentence!\n",
      "augmented 715 sentence!\n",
      "augmented 716 sentence!\n",
      "augmented 717 sentence!\n",
      "augmented 718 sentence!\n",
      "augmented 719 sentence!\n",
      "augmented 720 sentence!\n",
      "augmented 721 sentence!\n",
      "augmented 722 sentence!\n",
      "augmented 723 sentence!\n",
      "augmented 724 sentence!\n",
      "augmented 725 sentence!\n",
      "augmented 726 sentence!\n",
      "augmented 727 sentence!\n",
      "augmented 728 sentence!\n",
      "augmented 729 sentence!\n",
      "augmented 730 sentence!\n",
      "augmented 731 sentence!\n",
      "augmented 732 sentence!\n",
      "augmented 733 sentence!\n",
      "augmented 734 sentence!\n",
      "augmented 735 sentence!\n",
      "augmented 736 sentence!\n",
      "augmented 737 sentence!\n",
      "augmented 738 sentence!\n",
      "augmented 739 sentence!\n",
      "augmented 740 sentence!\n",
      "augmented 741 sentence!\n",
      "augmented 742 sentence!\n",
      "augmented 743 sentence!\n",
      "augmented 744 sentence!\n",
      "augmented 745 sentence!\n",
      "augmented 746 sentence!\n",
      "augmented 747 sentence!\n",
      "augmented 748 sentence!\n",
      "augmented 749 sentence!\n",
      "augmented 750 sentence!\n",
      "augmented 751 sentence!\n",
      "augmented 752 sentence!\n",
      "augmented 753 sentence!\n",
      "augmented 754 sentence!\n",
      "augmented 755 sentence!\n",
      "augmented 756 sentence!\n",
      "augmented 757 sentence!\n",
      "augmented 758 sentence!\n",
      "augmented 759 sentence!\n",
      "augmented 760 sentence!\n",
      "augmented 761 sentence!\n",
      "augmented 762 sentence!\n",
      "augmented 763 sentence!\n",
      "augmented 764 sentence!\n",
      "augmented 765 sentence!\n",
      "augmented 766 sentence!\n",
      "augmented 767 sentence!\n",
      "augmented 768 sentence!\n",
      "augmented 769 sentence!\n",
      "augmented 770 sentence!\n",
      "augmented 771 sentence!\n",
      "augmented 772 sentence!\n",
      "augmented 773 sentence!\n",
      "augmented 774 sentence!\n",
      "augmented 775 sentence!\n",
      "augmented 776 sentence!\n",
      "augmented 777 sentence!\n",
      "augmented 778 sentence!\n",
      "augmented 779 sentence!\n",
      "augmented 780 sentence!\n",
      "augmented 781 sentence!\n",
      "augmented 782 sentence!\n",
      "augmented 783 sentence!\n",
      "augmented 784 sentence!\n",
      "augmented 785 sentence!\n",
      "augmented 786 sentence!\n",
      "augmented 787 sentence!\n",
      "augmented 788 sentence!\n",
      "augmented 789 sentence!\n",
      "augmented 790 sentence!\n",
      "augmented 791 sentence!\n",
      "augmented 792 sentence!\n",
      "augmented 793 sentence!\n",
      "augmented 794 sentence!\n",
      "augmented 795 sentence!\n",
      "augmented 796 sentence!\n",
      "augmented 797 sentence!\n",
      "augmented 798 sentence!\n",
      "augmented 799 sentence!\n",
      "augmented 800 sentence!\n",
      "augmented 801 sentence!\n",
      "augmented 802 sentence!\n",
      "augmented 803 sentence!\n",
      "augmented 804 sentence!\n",
      "augmented 805 sentence!\n",
      "augmented 806 sentence!\n",
      "augmented 807 sentence!\n",
      "augmented 808 sentence!\n",
      "augmented 809 sentence!\n",
      "augmented 810 sentence!\n",
      "augmented 811 sentence!\n",
      "augmented 812 sentence!\n",
      "augmented 813 sentence!\n",
      "augmented 814 sentence!\n",
      "augmented 815 sentence!\n",
      "augmented 816 sentence!\n",
      "augmented 817 sentence!\n",
      "augmented 818 sentence!\n",
      "augmented 819 sentence!\n",
      "augmented 820 sentence!\n",
      "augmented 821 sentence!\n",
      "augmented 822 sentence!\n",
      "augmented 823 sentence!\n",
      "augmented 824 sentence!\n",
      "augmented 825 sentence!\n",
      "augmented 826 sentence!\n",
      "augmented 827 sentence!\n",
      "augmented 828 sentence!\n",
      "augmented 829 sentence!\n",
      "augmented 830 sentence!\n",
      "augmented 831 sentence!\n",
      "augmented 832 sentence!\n",
      "augmented 833 sentence!\n",
      "augmented 834 sentence!\n",
      "augmented 835 sentence!\n",
      "augmented 836 sentence!\n",
      "augmented 837 sentence!\n",
      "augmented 838 sentence!\n",
      "augmented 839 sentence!\n",
      "augmented 840 sentence!\n",
      "augmented 841 sentence!\n",
      "augmented 842 sentence!\n",
      "augmented 843 sentence!\n",
      "augmented 844 sentence!\n",
      "augmented 845 sentence!\n",
      "augmented 846 sentence!\n",
      "augmented 847 sentence!\n",
      "augmented 848 sentence!\n",
      "augmented 849 sentence!\n",
      "augmented 850 sentence!\n",
      "augmented 851 sentence!\n",
      "augmented 852 sentence!\n",
      "augmented 853 sentence!\n",
      "augmented 854 sentence!\n",
      "augmented 855 sentence!\n",
      "augmented 856 sentence!\n",
      "augmented 857 sentence!\n",
      "augmented 858 sentence!\n",
      "augmented 859 sentence!\n",
      "augmented 860 sentence!\n",
      "augmented 861 sentence!\n",
      "augmented 862 sentence!\n",
      "augmented 863 sentence!\n",
      "augmented 864 sentence!\n",
      "augmented 865 sentence!\n",
      "augmented 866 sentence!\n",
      "augmented 867 sentence!\n",
      "augmented 868 sentence!\n",
      "augmented 869 sentence!\n",
      "augmented 870 sentence!\n",
      "augmented 871 sentence!\n",
      "augmented 872 sentence!\n",
      "augmented 873 sentence!\n",
      "augmented 874 sentence!\n",
      "augmented 875 sentence!\n",
      "augmented 876 sentence!\n",
      "augmented 877 sentence!\n",
      "augmented 878 sentence!\n",
      "augmented 879 sentence!\n",
      "augmented 880 sentence!\n",
      "augmented 881 sentence!\n",
      "augmented 882 sentence!\n",
      "augmented 883 sentence!\n",
      "augmented 884 sentence!\n",
      "augmented 885 sentence!\n",
      "augmented 886 sentence!\n",
      "augmented 887 sentence!\n",
      "augmented 888 sentence!\n",
      "augmented 889 sentence!\n",
      "augmented 890 sentence!\n",
      "augmented 891 sentence!\n",
      "augmented 892 sentence!\n",
      "augmented 893 sentence!\n",
      "augmented 894 sentence!\n",
      "augmented 895 sentence!\n",
      "augmented 896 sentence!\n",
      "augmented 897 sentence!\n",
      "augmented 898 sentence!\n",
      "augmented 899 sentence!\n",
      "augmented 900 sentence!\n",
      "augmented 901 sentence!\n",
      "augmented 902 sentence!\n",
      "augmented 903 sentence!\n",
      "augmented 904 sentence!\n",
      "augmented 905 sentence!\n",
      "augmented 906 sentence!\n",
      "augmented 907 sentence!\n",
      "augmented 908 sentence!\n",
      "augmented 909 sentence!\n",
      "augmented 910 sentence!\n",
      "augmented 911 sentence!\n",
      "augmented 912 sentence!\n",
      "augmented 913 sentence!\n",
      "augmented 914 sentence!\n",
      "augmented 915 sentence!\n",
      "augmented 916 sentence!\n",
      "augmented 917 sentence!\n",
      "augmented 918 sentence!\n",
      "augmented 919 sentence!\n",
      "augmented 920 sentence!\n",
      "augmented 921 sentence!\n",
      "augmented 922 sentence!\n",
      "augmented 923 sentence!\n",
      "augmented 924 sentence!\n",
      "augmented 925 sentence!\n",
      "augmented 926 sentence!\n",
      "augmented 927 sentence!\n",
      "augmented 928 sentence!\n",
      "augmented 929 sentence!\n",
      "augmented 930 sentence!\n",
      "augmented 931 sentence!\n",
      "augmented 932 sentence!\n",
      "augmented 933 sentence!\n",
      "augmented 934 sentence!\n",
      "augmented 935 sentence!\n",
      "augmented 936 sentence!\n",
      "augmented 937 sentence!\n",
      "augmented 938 sentence!\n",
      "augmented 939 sentence!\n",
      "augmented 940 sentence!\n",
      "augmented 941 sentence!\n",
      "augmented 942 sentence!\n",
      "augmented 943 sentence!\n",
      "augmented 944 sentence!\n",
      "augmented 945 sentence!\n",
      "augmented 946 sentence!\n",
      "augmented 947 sentence!\n",
      "augmented 948 sentence!\n",
      "augmented 949 sentence!\n",
      "augmented 950 sentence!\n",
      "augmented 951 sentence!\n",
      "augmented 952 sentence!\n",
      "augmented 953 sentence!\n",
      "augmented 954 sentence!\n",
      "augmented 955 sentence!\n",
      "augmented 956 sentence!\n",
      "augmented 957 sentence!\n",
      "augmented 958 sentence!\n",
      "augmented 959 sentence!\n",
      "augmented 960 sentence!\n",
      "augmented 961 sentence!\n",
      "augmented 962 sentence!\n",
      "augmented 963 sentence!\n",
      "augmented 964 sentence!\n",
      "augmented 965 sentence!\n",
      "augmented 966 sentence!\n",
      "augmented 967 sentence!\n",
      "augmented 968 sentence!\n",
      "augmented 969 sentence!\n",
      "augmented 970 sentence!\n",
      "augmented 971 sentence!\n",
      "augmented 972 sentence!\n",
      "augmented 973 sentence!\n",
      "augmented 974 sentence!\n",
      "augmented 975 sentence!\n",
      "augmented 976 sentence!\n",
      "augmented 977 sentence!\n",
      "augmented 978 sentence!\n",
      "augmented 979 sentence!\n",
      "augmented 980 sentence!\n",
      "augmented 981 sentence!\n",
      "augmented 982 sentence!\n",
      "augmented 983 sentence!\n",
      "augmented 984 sentence!\n",
      "augmented 985 sentence!\n",
      "augmented 986 sentence!\n",
      "augmented 987 sentence!\n",
      "augmented 988 sentence!\n",
      "augmented 989 sentence!\n",
      "augmented 990 sentence!\n",
      "augmented 991 sentence!\n",
      "augmented 992 sentence!\n",
      "augmented 993 sentence!\n",
      "augmented 994 sentence!\n",
      "augmented 995 sentence!\n",
      "augmented 996 sentence!\n",
      "augmented 997 sentence!\n",
      "augmented 998 sentence!\n",
      "augmented 999 sentence!\n",
      "augmented 1000 sentence!\n",
      "augmented 1001 sentence!\n",
      "augmented 1002 sentence!\n",
      "augmented 1003 sentence!\n",
      "augmented 1004 sentence!\n",
      "augmented 1005 sentence!\n",
      "augmented 1006 sentence!\n",
      "augmented 1007 sentence!\n",
      "augmented 1008 sentence!\n",
      "augmented 1009 sentence!\n",
      "augmented 1010 sentence!\n",
      "augmented 1011 sentence!\n",
      "augmented 1012 sentence!\n",
      "augmented 1013 sentence!\n",
      "augmented 1014 sentence!\n",
      "augmented 1015 sentence!\n",
      "augmented 1016 sentence!\n",
      "augmented 1017 sentence!\n",
      "augmented 1018 sentence!\n",
      "augmented 1019 sentence!\n",
      "augmented 1020 sentence!\n",
      "augmented 1021 sentence!\n",
      "augmented 1022 sentence!\n",
      "augmented 1023 sentence!\n",
      "augmented 1024 sentence!\n",
      "augmented 1025 sentence!\n",
      "augmented 1026 sentence!\n",
      "augmented 1027 sentence!\n",
      "augmented 1028 sentence!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented 1029 sentence!\n",
      "augmented 1030 sentence!\n",
      "augmented 1031 sentence!\n",
      "augmented 1032 sentence!\n",
      "augmented 1033 sentence!\n",
      "augmented 1034 sentence!\n",
      "augmented 1035 sentence!\n",
      "augmented 1036 sentence!\n",
      "augmented 1037 sentence!\n",
      "augmented 1038 sentence!\n",
      "augmented 1039 sentence!\n",
      "augmented 1040 sentence!\n",
      "augmented 1041 sentence!\n",
      "augmented 1042 sentence!\n",
      "augmented 1043 sentence!\n",
      "augmented 1044 sentence!\n",
      "augmented 1045 sentence!\n",
      "augmented 1046 sentence!\n",
      "augmented 1047 sentence!\n",
      "augmented 1048 sentence!\n",
      "augmented 1049 sentence!\n",
      "augmented 1050 sentence!\n",
      "augmented 1051 sentence!\n",
      "augmented 1052 sentence!\n",
      "augmented 1053 sentence!\n",
      "augmented 1054 sentence!\n",
      "augmented 1055 sentence!\n",
      "augmented 1056 sentence!\n",
      "augmented 1057 sentence!\n",
      "augmented 1058 sentence!\n",
      "augmented 1059 sentence!\n",
      "augmented 1060 sentence!\n",
      "augmented 1061 sentence!\n",
      "augmented 1062 sentence!\n",
      "augmented 1063 sentence!\n",
      "augmented 1064 sentence!\n",
      "augmented 1065 sentence!\n",
      "augmented 1066 sentence!\n",
      "augmented 1067 sentence!\n",
      "augmented 1068 sentence!\n",
      "augmented 1069 sentence!\n",
      "augmented 1070 sentence!\n",
      "augmented 1071 sentence!\n",
      "augmented 1072 sentence!\n",
      "augmented 1073 sentence!\n",
      "augmented 1074 sentence!\n",
      "augmented 1075 sentence!\n",
      "augmented 1076 sentence!\n",
      "augmented 1077 sentence!\n",
      "augmented 1078 sentence!\n",
      "augmented 1079 sentence!\n",
      "augmented 1080 sentence!\n",
      "augmented 1081 sentence!\n",
      "augmented 1082 sentence!\n",
      "augmented 1083 sentence!\n",
      "augmented 1084 sentence!\n",
      "augmented 1085 sentence!\n",
      "augmented 1086 sentence!\n",
      "augmented 1087 sentence!\n",
      "augmented 1088 sentence!\n",
      "augmented 1089 sentence!\n",
      "augmented 1090 sentence!\n",
      "augmented 1091 sentence!\n",
      "augmented 1092 sentence!\n",
      "augmented 1093 sentence!\n",
      "augmented 1094 sentence!\n",
      "augmented 1095 sentence!\n",
      "augmented 1096 sentence!\n",
      "augmented 1097 sentence!\n",
      "augmented 1098 sentence!\n",
      "augmented 1099 sentence!\n",
      "augmented 1100 sentence!\n",
      "augmented 1101 sentence!\n",
      "augmented 1102 sentence!\n",
      "augmented 1103 sentence!\n",
      "augmented 1104 sentence!\n",
      "augmented 1105 sentence!\n",
      "augmented 1106 sentence!\n",
      "augmented 1107 sentence!\n",
      "augmented 1108 sentence!\n",
      "augmented 1109 sentence!\n",
      "augmented 1110 sentence!\n",
      "augmented 1111 sentence!\n",
      "augmented 1112 sentence!\n",
      "augmented 1113 sentence!\n",
      "augmented 1114 sentence!\n",
      "augmented 1115 sentence!\n",
      "augmented 1116 sentence!\n",
      "augmented 1117 sentence!\n",
      "augmented 1118 sentence!\n",
      "augmented 1119 sentence!\n",
      "augmented 1120 sentence!\n",
      "augmented 1121 sentence!\n",
      "augmented 1122 sentence!\n",
      "augmented 1123 sentence!\n",
      "augmented 1124 sentence!\n",
      "augmented 1125 sentence!\n",
      "augmented 1126 sentence!\n",
      "augmented 1127 sentence!\n",
      "augmented 1128 sentence!\n",
      "augmented 1129 sentence!\n",
      "augmented 1130 sentence!\n",
      "augmented 1131 sentence!\n",
      "augmented 1132 sentence!\n",
      "augmented 1133 sentence!\n",
      "augmented 1134 sentence!\n",
      "augmented 1135 sentence!\n",
      "augmented 1136 sentence!\n",
      "augmented 1137 sentence!\n",
      "augmented 1138 sentence!\n",
      "augmented 1139 sentence!\n",
      "augmented 1140 sentence!\n",
      "augmented 1141 sentence!\n",
      "augmented 1142 sentence!\n",
      "augmented 1143 sentence!\n",
      "augmented 1144 sentence!\n",
      "augmented 1145 sentence!\n",
      "augmented 1146 sentence!\n",
      "augmented 1147 sentence!\n",
      "augmented 1148 sentence!\n",
      "augmented 1149 sentence!\n",
      "augmented 1150 sentence!\n",
      "augmented 1151 sentence!\n",
      "augmented 1152 sentence!\n",
      "augmented 1153 sentence!\n",
      "augmented 1154 sentence!\n",
      "augmented 1155 sentence!\n",
      "augmented 1156 sentence!\n",
      "augmented 1157 sentence!\n",
      "augmented 1158 sentence!\n",
      "augmented 1159 sentence!\n",
      "augmented 1160 sentence!\n",
      "augmented 1161 sentence!\n",
      "augmented 1162 sentence!\n",
      "augmented 1163 sentence!\n",
      "augmented 1164 sentence!\n",
      "augmented 1165 sentence!\n",
      "augmented 1166 sentence!\n",
      "augmented 1167 sentence!\n",
      "augmented 1168 sentence!\n",
      "augmented 1169 sentence!\n",
      "augmented 1170 sentence!\n",
      "augmented 1171 sentence!\n",
      "augmented 1172 sentence!\n",
      "augmented 1173 sentence!\n",
      "augmented 1174 sentence!\n",
      "augmented 1175 sentence!\n",
      "augmented 1176 sentence!\n",
      "augmented 1177 sentence!\n",
      "augmented 1178 sentence!\n",
      "augmented 1179 sentence!\n",
      "augmented 1180 sentence!\n",
      "augmented 1181 sentence!\n",
      "augmented 1182 sentence!\n",
      "augmented 1183 sentence!\n",
      "augmented 1184 sentence!\n",
      "augmented 1185 sentence!\n",
      "augmented 1186 sentence!\n",
      "augmented 1187 sentence!\n",
      "augmented 1188 sentence!\n",
      "augmented 1189 sentence!\n",
      "augmented 1190 sentence!\n",
      "augmented 1191 sentence!\n",
      "augmented 1192 sentence!\n",
      "augmented 1193 sentence!\n",
      "augmented 1194 sentence!\n",
      "augmented 1195 sentence!\n",
      "augmented 1196 sentence!\n",
      "augmented 1197 sentence!\n",
      "augmented 1198 sentence!\n",
      "augmented 1199 sentence!\n",
      "augmented 1200 sentence!\n",
      "augmented 1201 sentence!\n",
      "augmented 1202 sentence!\n",
      "augmented 1203 sentence!\n",
      "augmented 1204 sentence!\n",
      "augmented 1205 sentence!\n",
      "augmented 1206 sentence!\n",
      "augmented 1207 sentence!\n",
      "augmented 1208 sentence!\n",
      "augmented 1209 sentence!\n",
      "augmented 1210 sentence!\n",
      "augmented 1211 sentence!\n",
      "augmented 1212 sentence!\n",
      "augmented 1213 sentence!\n",
      "augmented 1214 sentence!\n",
      "augmented 1215 sentence!\n",
      "augmented 1216 sentence!\n",
      "augmented 1217 sentence!\n",
      "augmented 1218 sentence!\n",
      "augmented 1219 sentence!\n",
      "augmented 1220 sentence!\n",
      "augmented 1221 sentence!\n",
      "augmented 1222 sentence!\n",
      "augmented 1223 sentence!\n",
      "augmented 1224 sentence!\n",
      "augmented 1225 sentence!\n",
      "augmented 1226 sentence!\n",
      "augmented 1227 sentence!\n",
      "augmented 1228 sentence!\n",
      "augmented 1229 sentence!\n",
      "augmented 1230 sentence!\n",
      "augmented 1231 sentence!\n",
      "augmented 1232 sentence!\n",
      "augmented 1233 sentence!\n",
      "augmented 1234 sentence!\n",
      "augmented 1235 sentence!\n",
      "augmented 1236 sentence!\n",
      "augmented 1237 sentence!\n",
      "augmented 1238 sentence!\n",
      "augmented 1239 sentence!\n",
      "augmented 1240 sentence!\n",
      "augmented 1241 sentence!\n",
      "augmented 1242 sentence!\n",
      "augmented 1243 sentence!\n",
      "augmented 1244 sentence!\n",
      "augmented 1245 sentence!\n",
      "augmented 1246 sentence!\n",
      "augmented 1247 sentence!\n",
      "augmented 1248 sentence!\n",
      "augmented 1249 sentence!\n",
      "augmented 1250 sentence!\n",
      "augmented 1251 sentence!\n",
      "augmented 1252 sentence!\n",
      "augmented 1253 sentence!\n",
      "augmented 1254 sentence!\n",
      "augmented 1255 sentence!\n",
      "augmented 1256 sentence!\n",
      "augmented 1257 sentence!\n",
      "augmented 1258 sentence!\n",
      "augmented 1259 sentence!\n",
      "augmented 1260 sentence!\n",
      "augmented 1261 sentence!\n",
      "augmented 1262 sentence!\n",
      "augmented 1263 sentence!\n",
      "augmented 1264 sentence!\n",
      "augmented 1265 sentence!\n",
      "augmented 1266 sentence!\n",
      "augmented 1267 sentence!\n",
      "augmented 1268 sentence!\n",
      "augmented 1269 sentence!\n",
      "augmented 1270 sentence!\n",
      "augmented 1271 sentence!\n",
      "augmented 1272 sentence!\n",
      "augmented 1273 sentence!\n",
      "augmented 1274 sentence!\n",
      "augmented 1275 sentence!\n",
      "augmented 1276 sentence!\n",
      "augmented 1277 sentence!\n",
      "augmented 1278 sentence!\n",
      "augmented 1279 sentence!\n",
      "augmented 1280 sentence!\n",
      "augmented 1281 sentence!\n",
      "augmented 1282 sentence!\n",
      "augmented 1283 sentence!\n",
      "augmented 1284 sentence!\n",
      "augmented 1285 sentence!\n",
      "augmented 1286 sentence!\n",
      "augmented 1287 sentence!\n",
      "augmented 1288 sentence!\n",
      "augmented 1289 sentence!\n",
      "augmented 1290 sentence!\n",
      "augmented 1291 sentence!\n",
      "augmented 1292 sentence!\n",
      "augmented 1293 sentence!\n",
      "augmented 1294 sentence!\n",
      "augmented 1295 sentence!\n",
      "augmented 1296 sentence!\n",
      "augmented 1297 sentence!\n",
      "augmented 1298 sentence!\n",
      "augmented 1299 sentence!\n",
      "augmented 1300 sentence!\n",
      "augmented 1301 sentence!\n",
      "augmented 1302 sentence!\n",
      "augmented 1303 sentence!\n",
      "augmented 1304 sentence!\n",
      "augmented 1305 sentence!\n",
      "augmented 1306 sentence!\n",
      "augmented 1307 sentence!\n",
      "augmented 1308 sentence!\n",
      "augmented 1309 sentence!\n",
      "augmented 1310 sentence!\n",
      "augmented 1311 sentence!\n",
      "augmented 1312 sentence!\n",
      "augmented 1313 sentence!\n",
      "augmented 1314 sentence!\n",
      "augmented 1315 sentence!\n",
      "augmented 1316 sentence!\n",
      "augmented 1317 sentence!\n",
      "augmented 1318 sentence!\n",
      "augmented 1319 sentence!\n",
      "augmented 1320 sentence!\n",
      "augmented 1321 sentence!\n",
      "augmented 1322 sentence!\n",
      "augmented 1323 sentence!\n",
      "augmented 1324 sentence!\n",
      "augmented 1325 sentence!\n",
      "augmented 1326 sentence!\n",
      "augmented 1327 sentence!\n",
      "augmented 1328 sentence!\n",
      "augmented 1329 sentence!\n",
      "augmented 1330 sentence!\n",
      "augmented 1331 sentence!\n",
      "augmented 1332 sentence!\n",
      "augmented 1333 sentence!\n",
      "augmented 1334 sentence!\n",
      "augmented 1335 sentence!\n",
      "augmented 1336 sentence!\n",
      "augmented 1337 sentence!\n",
      "augmented 1338 sentence!\n",
      "augmented 1339 sentence!\n",
      "augmented 1340 sentence!\n",
      "augmented 1341 sentence!\n",
      "augmented 1342 sentence!\n",
      "augmented 1343 sentence!\n",
      "augmented 1344 sentence!\n",
      "augmented 1345 sentence!\n",
      "augmented 1346 sentence!\n",
      "augmented 1347 sentence!\n",
      "augmented 1348 sentence!\n",
      "augmented 1349 sentence!\n",
      "augmented 1350 sentence!\n",
      "augmented 1351 sentence!\n",
      "augmented 1352 sentence!\n",
      "augmented 1353 sentence!\n",
      "augmented 1354 sentence!\n",
      "augmented 1355 sentence!\n",
      "augmented 1356 sentence!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented 1357 sentence!\n",
      "augmented 1358 sentence!\n",
      "augmented 1359 sentence!\n",
      "augmented 1360 sentence!\n",
      "augmented 1361 sentence!\n",
      "augmented 1362 sentence!\n",
      "augmented 1363 sentence!\n",
      "augmented 1364 sentence!\n",
      "augmented 1365 sentence!\n",
      "augmented 1366 sentence!\n",
      "augmented 1367 sentence!\n",
      "augmented 1368 sentence!\n",
      "augmented 1369 sentence!\n",
      "augmented 1370 sentence!\n",
      "augmented 1371 sentence!\n",
      "augmented 1372 sentence!\n",
      "augmented 1373 sentence!\n",
      "augmented 1374 sentence!\n",
      "augmented 1375 sentence!\n",
      "augmented 1376 sentence!\n",
      "augmented 1377 sentence!\n",
      "augmented 1378 sentence!\n",
      "augmented 1379 sentence!\n",
      "augmented 1380 sentence!\n",
      "augmented 1381 sentence!\n",
      "augmented 1382 sentence!\n",
      "augmented 1383 sentence!\n",
      "augmented 1384 sentence!\n",
      "augmented 1385 sentence!\n",
      "augmented 1386 sentence!\n",
      "augmented 1387 sentence!\n",
      "augmented 1388 sentence!\n",
      "augmented 1389 sentence!\n",
      "augmented 1390 sentence!\n",
      "augmented 1391 sentence!\n",
      "augmented 1392 sentence!\n",
      "augmented 1393 sentence!\n",
      "augmented 1394 sentence!\n",
      "augmented 1395 sentence!\n",
      "augmented 1396 sentence!\n",
      "augmented 1397 sentence!\n",
      "augmented 1398 sentence!\n",
      "augmented 1399 sentence!\n",
      "augmented 1400 sentence!\n",
      "augmented 1401 sentence!\n",
      "augmented 1402 sentence!\n",
      "augmented 1403 sentence!\n",
      "augmented 1404 sentence!\n",
      "augmented 1405 sentence!\n",
      "augmented 1406 sentence!\n",
      "augmented 1407 sentence!\n",
      "augmented 1408 sentence!\n",
      "augmented 1409 sentence!\n",
      "augmented 1410 sentence!\n",
      "augmented 1411 sentence!\n",
      "augmented 1412 sentence!\n",
      "augmented 1413 sentence!\n",
      "augmented 1414 sentence!\n",
      "augmented 1415 sentence!\n",
      "augmented 1416 sentence!\n",
      "augmented 1417 sentence!\n",
      "augmented 1418 sentence!\n",
      "augmented 1419 sentence!\n",
      "augmented 1420 sentence!\n",
      "augmented 1421 sentence!\n",
      "augmented 1422 sentence!\n",
      "augmented 1423 sentence!\n",
      "augmented 1424 sentence!\n",
      "augmented 1425 sentence!\n",
      "augmented 1426 sentence!\n",
      "augmented 1427 sentence!\n",
      "augmented 1428 sentence!\n",
      "augmented 1429 sentence!\n",
      "augmented 1430 sentence!\n",
      "augmented 1431 sentence!\n",
      "augmented 1432 sentence!\n",
      "augmented 1433 sentence!\n",
      "augmented 1434 sentence!\n",
      "augmented 1435 sentence!\n",
      "augmented 1436 sentence!\n",
      "augmented 1437 sentence!\n",
      "augmented 1438 sentence!\n",
      "augmented 1439 sentence!\n",
      "augmented 1440 sentence!\n",
      "augmented 1441 sentence!\n",
      "augmented 1442 sentence!\n",
      "augmented 1443 sentence!\n",
      "augmented 1444 sentence!\n",
      "augmented 1445 sentence!\n",
      "augmented 1446 sentence!\n",
      "augmented 1447 sentence!\n",
      "augmented 1448 sentence!\n",
      "augmented 1449 sentence!\n",
      "augmented 1450 sentence!\n",
      "augmented 1451 sentence!\n",
      "augmented 1452 sentence!\n",
      "augmented 1453 sentence!\n",
      "augmented 1454 sentence!\n",
      "augmented 1455 sentence!\n",
      "augmented 1456 sentence!\n",
      "augmented 1457 sentence!\n",
      "augmented 1458 sentence!\n",
      "augmented 1459 sentence!\n",
      "augmented 1460 sentence!\n",
      "augmented 1461 sentence!\n",
      "augmented 1462 sentence!\n",
      "augmented 1463 sentence!\n",
      "augmented 1464 sentence!\n",
      "augmented 1465 sentence!\n",
      "augmented 1466 sentence!\n",
      "augmented 1467 sentence!\n",
      "augmented 1468 sentence!\n",
      "augmented 1469 sentence!\n",
      "augmented 1470 sentence!\n",
      "augmented 1471 sentence!\n",
      "augmented 1472 sentence!\n",
      "augmented 1473 sentence!\n",
      "augmented 1474 sentence!\n",
      "augmented 1475 sentence!\n",
      "augmented 1476 sentence!\n",
      "augmented 1477 sentence!\n",
      "augmented 1478 sentence!\n",
      "augmented 1479 sentence!\n",
      "augmented 1480 sentence!\n",
      "augmented 1481 sentence!\n",
      "augmented 1482 sentence!\n",
      "augmented 1483 sentence!\n",
      "augmented 1484 sentence!\n",
      "augmented 1485 sentence!\n",
      "augmented 1486 sentence!\n",
      "augmented 1487 sentence!\n",
      "augmented 1488 sentence!\n",
      "augmented 1489 sentence!\n",
      "augmented 1490 sentence!\n",
      "augmented 1491 sentence!\n",
      "augmented 1492 sentence!\n",
      "augmented 1493 sentence!\n",
      "augmented 1494 sentence!\n",
      "augmented 1495 sentence!\n",
      "augmented 1496 sentence!\n",
      "augmented 1497 sentence!\n",
      "augmented 1498 sentence!\n",
      "augmented 1499 sentence!\n",
      "augmented 1500 sentence!\n",
      "augmented 1501 sentence!\n",
      "augmented 1502 sentence!\n",
      "augmented 1503 sentence!\n",
      "augmented 1504 sentence!\n",
      "augmented 1505 sentence!\n",
      "augmented 1506 sentence!\n",
      "augmented 1507 sentence!\n",
      "augmented 1508 sentence!\n",
      "augmented 1509 sentence!\n",
      "augmented 1510 sentence!\n",
      "augmented 1511 sentence!\n",
      "augmented 1512 sentence!\n",
      "augmented 1513 sentence!\n",
      "augmented 1514 sentence!\n",
      "augmented 1515 sentence!\n",
      "augmented 1516 sentence!\n",
      "augmented 1517 sentence!\n",
      "augmented 1518 sentence!\n",
      "augmented 1519 sentence!\n",
      "augmented 1520 sentence!\n",
      "augmented 1521 sentence!\n",
      "augmented 1522 sentence!\n",
      "augmented 1523 sentence!\n",
      "augmented 1524 sentence!\n",
      "augmented 1525 sentence!\n",
      "augmented 1526 sentence!\n",
      "augmented 1527 sentence!\n",
      "augmented 1528 sentence!\n",
      "augmented 1529 sentence!\n",
      "augmented 1530 sentence!\n",
      "augmented 1531 sentence!\n",
      "augmented 1532 sentence!\n",
      "augmented 1533 sentence!\n",
      "augmented 1534 sentence!\n",
      "augmented 1535 sentence!\n",
      "augmented 1536 sentence!\n",
      "augmented 1537 sentence!\n",
      "augmented 1538 sentence!\n",
      "augmented 1539 sentence!\n",
      "augmented 1540 sentence!\n",
      "augmented 1541 sentence!\n",
      "augmented 1542 sentence!\n",
      "augmented 1543 sentence!\n",
      "augmented 1544 sentence!\n",
      "augmented 1545 sentence!\n",
      "augmented 1546 sentence!\n",
      "augmented 1547 sentence!\n",
      "augmented 1548 sentence!\n",
      "augmented 1549 sentence!\n",
      "augmented 1550 sentence!\n",
      "augmented 1551 sentence!\n",
      "augmented 1552 sentence!\n",
      "augmented 1553 sentence!\n",
      "augmented 1554 sentence!\n",
      "augmented 1555 sentence!\n",
      "augmented 1556 sentence!\n",
      "augmented 1557 sentence!\n",
      "augmented 1558 sentence!\n",
      "augmented 1559 sentence!\n",
      "augmented 1560 sentence!\n",
      "augmented 1561 sentence!\n",
      "augmented 1562 sentence!\n",
      "augmented 1563 sentence!\n",
      "augmented 1564 sentence!\n",
      "augmented 1565 sentence!\n",
      "augmented 1566 sentence!\n",
      "augmented 1567 sentence!\n",
      "augmented 1568 sentence!\n",
      "augmented 1569 sentence!\n",
      "augmented 1570 sentence!\n",
      "augmented 1571 sentence!\n",
      "augmented 1572 sentence!\n",
      "augmented 1573 sentence!\n",
      "augmented 1574 sentence!\n",
      "augmented 1575 sentence!\n",
      "augmented 1576 sentence!\n",
      "augmented 1577 sentence!\n",
      "augmented 1578 sentence!\n",
      "augmented 1579 sentence!\n",
      "augmented 1580 sentence!\n",
      "augmented 1581 sentence!\n",
      "augmented 1582 sentence!\n",
      "augmented 1583 sentence!\n",
      "augmented 1584 sentence!\n",
      "augmented 1585 sentence!\n",
      "augmented 1586 sentence!\n",
      "augmented 1587 sentence!\n",
      "augmented 1588 sentence!\n",
      "augmented 1589 sentence!\n",
      "augmented 1590 sentence!\n",
      "augmented 1591 sentence!\n",
      "augmented 1592 sentence!\n",
      "augmented 1593 sentence!\n",
      "augmented 1594 sentence!\n",
      "augmented 1595 sentence!\n",
      "augmented 1596 sentence!\n",
      "augmented 1597 sentence!\n",
      "augmented 1598 sentence!\n",
      "augmented 1599 sentence!\n",
      "augmented 1600 sentence!\n",
      "augmented 1601 sentence!\n",
      "augmented 1602 sentence!\n",
      "augmented 1603 sentence!\n",
      "augmented 1604 sentence!\n",
      "augmented 1605 sentence!\n",
      "augmented 1606 sentence!\n",
      "augmented 1607 sentence!\n",
      "augmented 1608 sentence!\n",
      "augmented 1609 sentence!\n",
      "augmented 1610 sentence!\n",
      "augmented 1611 sentence!\n",
      "augmented 1612 sentence!\n",
      "augmented 1613 sentence!\n",
      "augmented 1614 sentence!\n",
      "augmented 1615 sentence!\n",
      "augmented 1616 sentence!\n",
      "augmented 1617 sentence!\n",
      "augmented 1618 sentence!\n",
      "augmented 1619 sentence!\n",
      "augmented 1620 sentence!\n",
      "augmented 1621 sentence!\n",
      "augmented 1622 sentence!\n",
      "augmented 1623 sentence!\n",
      "augmented 1624 sentence!\n",
      "augmented 1625 sentence!\n",
      "augmented 1626 sentence!\n",
      "augmented 1627 sentence!\n",
      "augmented 1628 sentence!\n",
      "augmented 1629 sentence!\n",
      "augmented 1630 sentence!\n",
      "augmented 1631 sentence!\n",
      "augmented 1632 sentence!\n",
      "augmented 1633 sentence!\n",
      "augmented 1634 sentence!\n",
      "augmented 1635 sentence!\n",
      "augmented 1636 sentence!\n",
      "augmented 1637 sentence!\n",
      "augmented 1638 sentence!\n",
      "augmented 1639 sentence!\n",
      "augmented 1640 sentence!\n",
      "augmented 1641 sentence!\n",
      "augmented 1642 sentence!\n",
      "augmented 1643 sentence!\n",
      "augmented 1644 sentence!\n",
      "augmented 1645 sentence!\n",
      "augmented 1646 sentence!\n",
      "augmented 1647 sentence!\n",
      "augmented 1648 sentence!\n",
      "augmented 1649 sentence!\n",
      "augmented 1650 sentence!\n",
      "augmented 1651 sentence!\n",
      "augmented 1652 sentence!\n",
      "augmented 1653 sentence!\n",
      "augmented 1654 sentence!\n",
      "augmented 1655 sentence!\n",
      "augmented 1656 sentence!\n",
      "augmented 1657 sentence!\n",
      "augmented 1658 sentence!\n",
      "augmented 1659 sentence!\n",
      "augmented 1660 sentence!\n",
      "augmented 1661 sentence!\n",
      "augmented 1662 sentence!\n",
      "augmented 1663 sentence!\n",
      "augmented 1664 sentence!\n",
      "augmented 1665 sentence!\n",
      "augmented 1666 sentence!\n",
      "augmented 1667 sentence!\n",
      "augmented 1668 sentence!\n",
      "augmented 1669 sentence!\n",
      "augmented 1670 sentence!\n",
      "augmented 1671 sentence!\n",
      "augmented 1672 sentence!\n",
      "augmented 1673 sentence!\n",
      "augmented 1674 sentence!\n",
      "augmented 1675 sentence!\n",
      "augmented 1676 sentence!\n",
      "augmented 1677 sentence!\n",
      "augmented 1678 sentence!\n",
      "augmented 1679 sentence!\n",
      "augmented 1680 sentence!\n",
      "augmented 1681 sentence!\n",
      "augmented 1682 sentence!\n",
      "augmented 1683 sentence!\n",
      "augmented 1684 sentence!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented 1685 sentence!\n",
      "augmented 1686 sentence!\n",
      "augmented 1687 sentence!\n",
      "augmented 1688 sentence!\n",
      "augmented 1689 sentence!\n",
      "augmented 1690 sentence!\n",
      "augmented 1691 sentence!\n",
      "augmented 1692 sentence!\n",
      "augmented 1693 sentence!\n",
      "augmented 1694 sentence!\n",
      "augmented 1695 sentence!\n",
      "augmented 1696 sentence!\n",
      "augmented 1697 sentence!\n",
      "augmented 1698 sentence!\n",
      "augmented 1699 sentence!\n",
      "augmented 1700 sentence!\n",
      "augmented 1701 sentence!\n",
      "augmented 1702 sentence!\n",
      "augmented 1703 sentence!\n",
      "augmented 1704 sentence!\n",
      "augmented 1705 sentence!\n",
      "augmented 1706 sentence!\n",
      "augmented 1707 sentence!\n",
      "augmented 1708 sentence!\n",
      "augmented 1709 sentence!\n",
      "augmented 1710 sentence!\n",
      "augmented 1711 sentence!\n",
      "augmented 1712 sentence!\n",
      "augmented 1713 sentence!\n",
      "augmented 1714 sentence!\n",
      "augmented 1715 sentence!\n",
      "augmented 1716 sentence!\n",
      "augmented 1717 sentence!\n",
      "augmented 1718 sentence!\n",
      "augmented 1719 sentence!\n",
      "augmented 1720 sentence!\n",
      "augmented 1721 sentence!\n",
      "augmented 1722 sentence!\n",
      "augmented 1723 sentence!\n",
      "augmented 1724 sentence!\n",
      "augmented 1725 sentence!\n",
      "augmented 1726 sentence!\n",
      "augmented 1727 sentence!\n",
      "augmented 1728 sentence!\n",
      "augmented 1729 sentence!\n",
      "augmented 1730 sentence!\n",
      "augmented 1731 sentence!\n",
      "augmented 1732 sentence!\n",
      "augmented 1733 sentence!\n",
      "augmented 1734 sentence!\n",
      "augmented 1735 sentence!\n",
      "augmented 1736 sentence!\n",
      "augmented 1737 sentence!\n",
      "augmented 1738 sentence!\n",
      "augmented 1739 sentence!\n",
      "balanced training sentences is now Counter({0: 6573, 1: 3480})\n",
      "10053\n",
      "test set is Counter({0: 731, 1: 193})\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "#make training data - word embeds \n",
    "\n",
    "skills_augment_embed = oversample_skills_embeds(skills)\n",
    "balanced_augment_embed = skills_augment_embed + [(train, label) for train, label in zip(X_train, y_train) if label == 0]\n",
    "\n",
    "X_train_oversample_embeds = [x[0] for x in balanced_augment_embed]\n",
    "y_train_oversample_embeds = [x[1] for x in balanced_augment_embed]\n",
    "\n",
    "print(f'balanced training sentences is now {Counter(y_train_oversample_embeds)}')\n",
    "print(len(y_train_oversample_embeds))\n",
    "\n",
    "print(f'test set is {Counter(y_test)}')\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2801c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-05 16:27:00,961 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:27:00,962 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:27:00,963 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 16:27:00,964 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:27:01,274 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 10053 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 16:27:01,275 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 16:27:01,275 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 16:27:08,494 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 252\n",
      "Took 80.85422801971436 seconds\n"
     ]
    }
   ],
   "source": [
    "# vectorise new training\n",
    "X_train_vec = sent_class.fit_transform(X_train_oversample_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54775317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:28:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       731\n",
      "           1       0.66      0.73      0.69       193\n",
      "\n",
      "    accuracy                           0.86       924\n",
      "   macro avg       0.79      0.81      0.80       924\n",
      "weighted avg       0.87      0.86      0.87       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run balanced data on xgboost\n",
    "\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(X_train_vec, y_train_oversample_embeds)\n",
    "predict = xgb.predict(X_test_vec)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b04ddc",
   "metadata": {},
   "source": [
    "## Experiment No. 8 - Balance training data - oversample 1 class (word synonyms) + under sample 0 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5802a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced training sentences is now Counter({1: 3480, 0: 3480})\n",
      "6960\n",
      "test set is Counter({0: 731, 1: 193})\n",
      "924\n",
      "2021-08-05 16:29:41,131 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:29:41,132 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:29:41,132 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 16:29:41,134 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:29:41,453 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 6960 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 16:29:41,453 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 16:29:41,453 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 16:29:48,374 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 174\n",
      "Took 59.75308322906494 seconds\n",
      "[16:30:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87       731\n",
      "           1       0.53      0.76      0.62       193\n",
      "\n",
      "    accuracy                           0.81       924\n",
      "   macro avg       0.73      0.79      0.74       924\n",
      "weighted avg       0.84      0.81      0.82       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversample_undersample_training_syns = skills_augment + random.sample([(train, label) for train, label in zip(X_train, y_train) if label == 0], \n",
    "                                     len(skills_augment))\n",
    "\n",
    "X_train_overunder_syns = [x[0] for x in oversample_undersample_training_syns]\n",
    "y_train_overunder_syns = [x[1] for x in oversample_undersample_training_syns]\n",
    "\n",
    "print(f'balanced training sentences is now {Counter(y_train_overunder_syns)}')\n",
    "print(len(y_train_overunder_syns))\n",
    "print(f'test set is {Counter(y_test)}')\n",
    "print(len(y_test))\n",
    "\n",
    "X_train_vec = sent_class.fit_transform(X_train_overunder_syns)\n",
    "\n",
    "# run balanced data on xgboost\n",
    "\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(X_train_vec, y_train_overunder_syns)\n",
    "predict = xgb.predict(X_test_vec)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574e7f6",
   "metadata": {},
   "source": [
    "## Experiment No. 9 - Balance training data - oversample 1 class (word embeds) + under sample 0 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9aa897b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced training sentences is now Counter({1: 3480, 0: 3480})\n",
      "6960\n",
      "test set is Counter({0: 731, 1: 193})\n",
      "924\n",
      "2021-08-05 16:31:37,486 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:31:37,487 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:31:37,487 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 16:31:37,488 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 16:31:37,832 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 6960 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 16:31:37,832 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 16:31:37,832 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 16:31:44,866 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 174\n",
      "Took 64.11040592193604 seconds\n",
      "[16:32:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.86       731\n",
      "           1       0.51      0.81      0.63       193\n",
      "\n",
      "    accuracy                           0.80       924\n",
      "   macro avg       0.73      0.80      0.75       924\n",
      "weighted avg       0.85      0.80      0.81       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversample_undersample_training_embeds = skills_augment_embed + random.sample([(train, label) for train, label in zip(X_train, y_train) if label == 0], \n",
    "                                     len(skills_augment_embed))\n",
    "X_train_overunder_embeds = [x[0] for x in oversample_undersample_training_embeds]\n",
    "y_train_overunder_embeds = [x[1] for x in oversample_undersample_training_embeds]\n",
    "\n",
    "print(f'balanced training sentences is now {Counter(y_train_overunder_embeds)}')\n",
    "print(len(y_train_overunder_embeds))\n",
    "print(f'test set is {Counter(y_test)}')\n",
    "print(len(y_test))\n",
    "\n",
    "X_train_vec = sent_class.fit_transform(X_train_overunder_embeds)\n",
    "\n",
    "# run balanced data on xgboost\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(X_train_vec, y_train_overunder_embeds)\n",
    "predict = xgb.predict(X_test_vec)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99409b7",
   "metadata": {},
   "source": [
    "## Experiment No. 10 - use one hot encoding of verb positionality w/o balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d40a033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_verbs(text_data, max_len = 600):\n",
    "    verb_pos = []\n",
    "    for text in text_data:\n",
    "        pos_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "        verb_pos.append([tag[1].count('VBG') for tag in pos_tags])\n",
    "    \n",
    "    # to array \n",
    "    verb_array = np.array(list(zip_longest(*verb_pos, fillvalue=0))).T\n",
    "    \n",
    "    return np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in verb_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f262bc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# stack verb counts \n",
    "train_verb_stack = np.hstack((X_train_vec, count_verbs(X_train)))\n",
    "test_verb_stack = np.hstack((X_test_vec, count_verbs(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "099de811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       731\n",
      "           1       0.74      0.55      0.63       193\n",
      "\n",
      "    accuracy                           0.86       924\n",
      "   macro avg       0.81      0.75      0.77       924\n",
      "weighted avg       0.86      0.86      0.86       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run xgboost\n",
    "\n",
    "xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "xgb.fit(train_verb_stack, y_train)\n",
    "predict = xgb.predict(test_verb_stack)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ab0b3",
   "metadata": {},
   "source": [
    "## Experiment No. 11 - adjust probability threshold to 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b5e638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform second level one up reviews on those loans recommended for approval above someones lending authority\n",
      "driven development with chai junit karma mocha sinon behaviour\n",
      "working as part of a supportive multidisciplinary team you will be expected to carry out ongoing treatment programmes for a mixed caseload of mainly older people\n",
      "can you help college students with subjects like construction sports painting and english\n",
      "my client is dynamic fast paced and in an exciting environment\n",
      "supporting the financial control environment accounts payable adhoc project work as and when required\n",
      "report and take necessary action for any incidents of accidents fire theft loss damage or other irregularities to deal with any complaints issues from customers immediately and report directly to your supervisor to be flexible to work additional number in order to cover holiday and sickness within the team\n",
      "organization skills\n",
      "as senior bi developer you will be responsible for developing enhancing and keeping up to date with new data solutions\n",
      "the successful applicant will need to be reliable enthusiastic and calm under pressure with the ability to deal with highly confidential and sensitive matters with compassion and understanding\n",
      "it also means managing a home voting staying healthy and being able to pursue employment or volunteering opportunities\n",
      "indepth knowledge and experience of react native mobile framework ios objective\n",
      "commissioning and management of technical partners necessary to support rapid web app development to support lifelong learning of digital skills\n",
      "previous experience as an application developer or application support analyst and good knowledge of microsoft office project management and atlassian jira as well as technical skills in the below net\n",
      "to undertake advocacy in relation to the more challenging serious and sensitive cases in the magstrates courts including youth courts ngap court lists and multiday trials\n",
      "youll learn about presentation availability commerciality and experience as well as receive the tools and support you need to make a big success of your role\n",
      "implements and monitors patient care plans\n",
      "assisting with domestic duties ensuring a clean tidy home\n",
      "qts is desirable\n",
      "increasing employee digital confidence and capability across the sector is key strengthening digital leadership at all levels enabling a thriving ecosystem of digital data informatics specialists developing skills across the workforce from the most basic to advanced\n",
      "reasonable adjustments\n",
      "matched to young peoples different needs abilities and interests\n",
      "engineering\n",
      "this varied role with its wide remit involves supporting quality assurance across the group by monitoring performance and logging incidents as well as involvement in creative copywriting for the companys customer focused communications\n",
      "sound good\n",
      "in some roles youll specialise finetuning your knowledge and honing your expertise in a particular area\n",
      "working directly with the front facing relationship management and client service teams as well as back office operations teams you will ensure that all new updated client relationships undergo the appropriate levels of client identification and verification due diligence sanction screening and documentary approvals to ensure validation and completeness of onboarding packs prior to submission for account opening\n",
      "reporting to the area manager the main responsibilities of the role are to\n",
      "you will also require extensive knowledge of client money rules and regulations and an aptitude for data analysis\n",
      "reporting to an approachable and supportive management team alongside a group of friendly teammates\n",
      "handling exchange and completion and post completion matters\n",
      "are you skilled in your craft passionate about the quality of your work friendly and professional\n",
      "as part of the open digital team youll be responsible for the following activities\n",
      "the engineering design section has several principal engineers experienced in highways project management and contract administration\n",
      "the successful applicant will need to be reliable enthusiastic and calm under pressure with the ability to deal with highly confidential and sensitive matters with compassion and understanding\n",
      "more often this will be through your projects and interactions with the stakeholders\n",
      "whether out on the shop floor or connecting with customers on the tills\n",
      "and you will need to be willing to commit the time and energy necessary to building relationships and supporting the business of the association\n"
     ]
    }
   ],
   "source": [
    "#print false positives\n",
    "\n",
    "predicted_probabilities = xgb.predict_proba(test_verb_stack)\n",
    "for sentence, probability, test_label, predicted_label in zip(X_test, predicted_probabilities, y_test, predict):\n",
    "    if test_label == 0 and predicted_label == 1:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5249e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sent is: is happy to work remotely\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.523\n",
      "the sent is: working as part of a supportive multidisciplinary team you will be expected to carry out ongoing treatment programmes for a mixed caseload of mainly older people\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.491\n",
      "the sent is: the successful applicant will be joining a really friendly team within a successful and dyanmic organisation\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.639\n",
      "the sent is: reviews of expenses purchasing card fuel cards statements if any to ensure compliance with policies\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.51\n",
      "the sent is: key requirementsstrong experience in digital marketing specifically ppc campaigns strong commercial acumen experience creating and managing ppc campaigns including social media channels and google adsexperience of using web analytics tools\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.533\n",
      "the sent is: it also means managing a home voting staying healthy and being able to pursue employment or volunteering opportunities\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.481\n",
      "the sent is: in conjunction with home manager networking and sharing pr stories within the local community to raise the profile of the brand of four seasons health care and the home\n",
      "the predicted label is: 1\n",
      "the true label is: 1\n",
      "the probability is: 0.487\n",
      "the sent is: commissioning and management of technical partners necessary to support rapid web app development to support lifelong learning of digital skills\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.471\n",
      "the sent is: relevant degree in a quantitative field eg finance computer science statistics mathematics engineering economics\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.55\n",
      "the sent is: this is a shortterm role and therefore we need someone to hit the ground running and start contributing straight away\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.593\n",
      "the sent is: to undertake advocacy in relation to the more challenging serious and sensitive cases in the magstrates courts including youth courts ngap court lists and multiday trials\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.472\n",
      "the sent is: requirements completed a recognised engineering apprenticeship be an experienced maintenance engineer ideally electrical bias however mechanical will also be considered as long as they have\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.545\n",
      "the sent is: helping communicators around the world capture headlines go viral and drive real measurable business impact\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.569\n",
      "the sent is: your team consists of experienced customer service representatives and by teaming up with other operational and supportive departments within the organisation a high service level can be achieved with the shared aim of achieving the very best for our customers\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.575\n",
      "the sent is: pimmdm inhouse project manager mf fulltime unlimited asap within ngk ratingen germany\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.579\n",
      "the sent is: it is essential that you are able to consult effectively with citizens and local communities and gain commitment from a wide range of people at all levels across the council voluntary and community groups and other partners\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.506\n",
      "the sent is: basic auto mechanic skills required\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.592\n",
      "the sent is: engineering\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.487\n",
      "the sent is: establishing and supporting the development of regional and local networks of digital motivators and champions from within the health and social care sectors\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.514\n",
      "the sent is: please submit your cv which should outline your relevant skills and experience\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.575\n",
      "the sent is: excell supply is committed to the safeguarding and welfare of children\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.514\n",
      "the sent is: completing gas safety checks\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.563\n",
      "the sent is: gas installations engineers working on our premium fleet provide a number class home delivery installation and collection service to our customers working a number on number off shift pattern there is ample opportunity to make the most of all of your down time and achieve a great work life balance\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.563\n",
      "the sent is: ability to analyze bank statements for any recent large deposits and possible other debts that might impact atr\n",
      "the predicted label is: 1\n",
      "the true label is: 1\n",
      "the probability is: 0.466\n",
      "the sent is: extensive software testing experience\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.599\n",
      "the sent is: the planning lawyer will be a qualified solicitor or equivalent and should be able to hit the ground running as such local authority experience is preferable\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.514\n",
      "the sent is: the ideal candidate will have experience in the investments space however those with banking or financial services will also be considered\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.618\n",
      "the sent is: visual studio and visual studio team servicesazure dev\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.552\n",
      "the sent is: are you skilled in your craft passionate about the quality of your work friendly and professional\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.492\n",
      "the sent is: key experience needed\n",
      "the predicted label is: 0\n",
      "the true label is: 0\n",
      "the probability is: 0.537\n",
      "the sent is: as part of the open digital team youll be responsible for the following activities\n",
      "the predicted label is: 1\n",
      "the true label is: 0\n",
      "the probability is: 0.464\n",
      "the sent is: knowledge of the ks curriculum\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.541\n",
      "the sent is: maintain the outside cleanliness and tidiness of the store eg trolley bays and the car park clean the warehouse area including the waste and recycling sections\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.543\n",
      "the sent is: support other teams in delivery of overall programme what we require from the candidate\n",
      "the predicted label is: 0\n",
      "the true label is: 1\n",
      "the probability is: 0.586\n",
      "the sent is: leading with motivation\n",
      "the predicted label is: 1\n",
      "the true label is: 1\n",
      "the probability is: 0.464\n"
     ]
    }
   ],
   "source": [
    "# look into edge case sentences\n",
    "\n",
    "edge_cases = []\n",
    "for index, probabilities in enumerate(predicted_probabilities):\n",
    "    if 0.45 < probabilities[0] < 0.65:\n",
    "        edge_cases.append((index, probabilities[0], probabilities[1]))\n",
    "\n",
    "for i in edge_cases:\n",
    "    print(f'the sent is: {X_test[i[0]]}')\n",
    "    print(f'the predicted label is: {predict[i[0]]}')\n",
    "    print(f'the true label is: {y_test[i[0]]}')\n",
    "    print(f'the probability is: {str(round(i[1], 3))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd9afd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       731\n",
      "           1       0.71      0.61      0.66       193\n",
      "\n",
      "    accuracy                           0.87       924\n",
      "   macro avg       0.80      0.77      0.79       924\n",
      "weighted avg       0.86      0.87      0.86       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adjust threshold\n",
    "\n",
    "y_pred_adjusted = []\n",
    "for prob in predicted_probabilities:\n",
    "    y_pred_adjusted.append(int(np.where(prob[1] > 0.40, 1, 0)))\n",
    "    \n",
    "print(classification_report(y_test, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc16fe",
   "metadata": {},
   "source": [
    "## Experiment No. 12 - adjust probability threshold to 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d514bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       731\n",
      "           1       0.77      0.52      0.62       193\n",
      "\n",
      "    accuracy                           0.87       924\n",
      "   macro avg       0.82      0.74      0.77       924\n",
      "weighted avg       0.86      0.87      0.86       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adjust threshold\n",
    "\n",
    "y_pred_adjusted = []\n",
    "for prob in predicted_probabilities:\n",
    "    y_pred_adjusted.append(int(np.where(prob[1] > 0.60, 1, 0)))\n",
    "    \n",
    "print(classification_report(y_test, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57d91fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7fd5b0090d90>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1xklEQVR4nO3dd3xUZfb48c9JgQQINQExhVBCDUUIXaSJoChYWAVXXSuu4urqqotlLVgWha+yllWxAPpzsawNEaUISFmQJkVCDwFCJ6RBenJ+f8wwBBLIAJlMkjnv12temXvvc+89l4Q589z7FFFVjDHG+C4/bwdgjDHGuywRGGOMj7NEYIwxPs4SgTHG+DhLBMYY4+MCvB3AuQoNDdXo6Ghvh2GMMZXK6tWrj6hqWEnbKl0iiI6OZtWqVd4OwxhjKhUR2XWmbXZryBhjfJwlAmOM8XGWCIwxxsdZIjDGGB9nicAYY3ycxxKBiHwkIodE5PczbBcReUNEtovIehHp7KlYjDHGnJknawRTgSFn2X4lEON8jQbe8WAsxhhjzsBj/QhUdZGIRJ+lyHDgY3WMg71cROqKSGNV3e+JeFYmHmXx1sMAXN3xYlo2CvHEaYwxptLxZoeycGBPkeUk57piiUBERuOoNRAVFXVeJ1uzK4U3F2xHFQ6m5/DKiA7ndRxjjKlqKsXDYlWdrKpxqhoXFlZiD+lS3du3OTv/OZTwusEU2GQ8xhjj4s1EsBeILLIc4VxnjDGmHHkzEcwAbnO2HuoBpHnq+YAxxpgz89gzAhGZDvQDQkUkCXgWCARQ1XeBWcBVwHYgE7jDU7EYY4w5M0+2GhpVynYFxnjq/MYYY9xTKR4WG2OM8RxLBMYY4+MsERhjjI+zRGCMMT7OEoExxvg4SwTGGOPjLBEYY4yPs0RgjDE+zhKBMcb4OEsExhjj4ywRGGOMj7NEYIwxPs4SgTHG+DhLBMYY4+MsERhjjI+zRGCMMT7OEoExxvg4SwTGGOPjLBEYY4yPs0RgjDE+zqOJQESGiMgWEdkuImNL2N5ERH4WkfUislBEIjwZjzHGmOI8lghExB94G7gSaAuMEpG2pxWbCHysqh2AccA/PRWPMcaYknmyRtAN2K6qCaqaC3wGDD+tTFtgvvP9ghK2G2OM8TBPJoJwYE+R5STnuqLWAdc7318HhIhIg9MPJCKjRWSViKw6fPiwR4I1xhhf5e2HxY8CfUXkN6AvsBcoOL2Qqk5W1ThVjQsLCyvvGI0xpkoL8OCx9wKRRZYjnOtcVHUfzhqBiNQCblDVVA/GZIwx5jSerBGsBGJEpKmIVANGAjOKFhCRUBE5EcMTwEcejMcYY0wJPJYIVDUfeACYDWwCvlDVjSIyTkSGOYv1A7aIyFagEfCSp+IxxhhTMk/eGkJVZwGzTlv3TJH3/wX+68kYjDHGnJ23HxYbY4zxMksExhjj4ywRGGOMj7NEYIwxPs4SgTHG+DhLBMYY4+MsERhjjI+zRGCMMT7OEoExxvg4SwTGGOPjLBEYY4yPs0RgjDE+zqODzlUWBYVKoSr+Ivj5ibfDMcaYcuXziSA1M5fLXl1AenY+nSLr8u2Y3t4OyRhjypXP3xo6ejyX9Ox8ggL92HnkuLfDqdKO5eRz9Hgu+QWF3g7FGFOEz9cITggO9KdQvR1F1fT73jS+WpPElKWJAPRvFcaUO7p5NyhjjIslAnNWufmF5BcWUj3AH/9zfH6ScPgYr/60hZ82HnCtq+bvx/607LIO0xhzASwR+DBV5Y2ft7MvNYtmYTW5t29z17as3AJW7TrKrR+uAKBrdD2+/HMv1/YvV+1hVWIKIUEBPDakFdUD/AHIzitgw940bvtwBVl5BQBE1AtmVLcobunRhMe+XMfuo5nleJXGmNJYIvBRy3Yk8+GSBOZtOuRad0+fZiiwZncK936ymqPHc13bTnx4r9mdwrjv41m7J9W1bXincGLDazN9xR5e+iGe47mOBFCregB/H9KKW3tGl8clGWPOkyUCH5OdV8CrP23ho6U7AWgWWpPwesEs3naEzQcy+GjpTv67OgmA6gF+vHNLZ35Yf4Cvf0ui9/j57E3NAqBnswbENKrFx8t28f36fTz65Tq2HMxw7Tf1jm5cElWXoEB/71yoMcZtHk0EIjIE+BfgD3ygquNP2x4FTAPqOsuMdU54bzxg/uaDvDl/O7/tTgXg5u5RvHxde16YGc/ibUe46o3FrrJv3XwJfVqEUadGIHdOXQXA3tQsggP9efKq1tzaM5oPFicAMHmR42fHyLqMv749rRqFWH8MYyoRjyUCEfEH3gYGAUnAShGZoarxRYo9DXyhqu+ISFtgFhDtqZiquuUJycxcv49Afz/u69uchrWDAEjPzuO/q5J4f3EC+9OyaRhSnc/v7UnT0JoA/OfX3a5jXBl7Ec9e046L6gQVO/7469szJPYi6taoBsChjBzAUQP418hLGBJ7kacvEYCU47m8tWA7OfkFDGp7EX1bhpXLeY2pqjxZI+gGbFfVBAAR+QwYDhRNBArUdr6vA+zzYDzF7E/LYvG2I+V5yguSlpnHsoRk/AR6tQilVnXHr6+wUFmy/Qivzd3qunffqlEII7tFsWl/Ol+uSnLdCrqjdzTPXtPulOO2aRzCmt2pLHy0H1H1axT7Nr907AD2p2YRF13/lPV/u6IlR4/nMmFEB0TOrQawKvEoR47lEFm/Bu0urlNq+eM5+SzdfoTH/ruetKw81/qEw8ctERhzgTyZCMKBPUWWk4Dup5V5DpgjIn8BagKXl3QgERkNjAaIiooqswD/8e1G5m06CECd4EBSMvNK2cO7/v3Ldt77xXEb5rHBrRjTvwUAv+1J4baPHK17ohvUIDE5k0KFPUczufn95aRk5uHvJyz5e38uql38m/6Xf+5FVl6BK7GcLrxuMOF1g4utrx7gz8Q/dDyna1iReJTUzDxGvLsMgJCgADY8N/iM5VWVl37YxAdLdrrW+Qk8OrgV8zcdQk/r+5Gencf/th9hytJEAvyFO3o15fK2jc4pRmN8jbcfFo8Cpqrq/4lIT+ATEYlV1VO6nqrqZGAyQFxcXJl1+8rJL6D1RSH8+4+dmfa/RL5dW64VErdl5RZw+5QVbD6QQUj1ADJy8slxNs388yerXe30X7uxIx0i6nL5a7/wwZIEnvxmAwCjukXx0MCYEm/3APj7yRmTQFlLdSbbkKAAwmpVJ+HIcQb830KOZOTQqHYQ3z3Qm+BAf5btSGbCnC2u5xkA7cPrMOEPHWgRVosAfz8Wbj7s2rb1YAZfr9nLu7/sOOV81fz9WLztMNl5hfRv3bDcbl8ZU5l48n//XiCyyHKEc11RdwFDAFR1mYgEAaHAIcpJjWr+NAurdc63NspLyvFc5m8+xK87j9Ixog5Xtm/MKz9tJik1iye/2cDS7UeIaViLG+MiubrDxexPc7TqSTh8nEB/4dURHejXsiH1albz8pU4jL6sGQmHj/PBn+Jo+sQPgCNWgPTsY3yybBcz1+9nw9401z6PXtGSkd2iCK1V/ZRjJR/PYcfh48Q+O5tjOfmu9YPbNeJPvaK5+f1fWbClSLI4lFFiIjiUnk1eoXJR7aBz7jRnTFXgyUSwEogRkaY4EsBI4ObTyuwGBgJTRaQNEAQcxrg88fUG1zf+BwbEMKhtI8b/uJmv1zhyaqPa1bmlRxP+1Csa4JTbW09d1YbrLoko95jP5smr2rjexzWpx8rEFD64LY5Zv+/n6zV7+eePmwEIrVWdl66LpUezBtQJDizxWDucCeRYTj7RDWpwf/8W3BgXWazc88PaMTf+4CnJAuCzFbuZvCiBBOcYU3/q2YTRfZuTmZPPxXWDqVmklrQhKY2dycdpULMavVuEXtg/gjEVjMcSgarmi8gDwGwcTUM/UtWNIjIOWKWqM4C/Ae+LyMM4Hhzfrnr6XV/flJaZx+vzHA9/YxrWYsIfOtIh/NSHqiIw5+G+p3xQtmhYC4BXb+jAjV2LfyhWJEV7Kn+3znFb7pKouvxtUCsujSn9wzYo0I/svEJmPNCb9uF1itXqEscPdb3/ebOjkjkv/iCTFyWwIvFoseNNW7aLact2AdC9aX2eH96Ob3/bx/zNB9l68Jir3IonB7paZBlTFXj0xrCzT8Cs09Y9U+R9PGDjPpdg1a6jTP1fIvVrVmNAm4Z0iqxbrMwX9/Ys9m25VvWAUz4AK4sJIzrQp0XoOSWvzS9c6XbZRVsdFc27P17lWvdA/xbc1DWSyPo1iB7ruE0VHOhPVl4Bv+48ypBJJ/tVDGjdkPVJqRw5lku3l3/mwYExPDKopdvnN6Yi8/bDYlOCp77ZwJx4R2umaXd0o31E8eaV9/drTtfTmnNWZkGB/uVSg+kTE8oD/VvQIaIuwdVO9noeN7wdEfWCGdC6kSsptGoUwh97RNG/VUMi69dg0rytTJq3DYANSakej9WY8mKJoIIpLFQWbD5EUKAft/ZoQkyjWsXKVMZv/N428y+XkltQSOeoeiVuv63IeEjfP3ApqVm59Ik5tX/CXy9vyV8vb8mwt5Z4MlRjyp0lggpk9sYD3P/pGgoKlZviInnh2lhvh1RlxIaX3mnthJJqYEWtT3K0aLp72kr+eX0HwkKqn7X8CTn5BWw9cIw3528jv1CJrBfMc8PaVdgWa8Z3WCKoIPIKCtl6IIOCQuWB/i249pKLvR2SKcW8TYe4ZV8a/Vo1PGOZzNx8thzIYNK8bfyytXiDuJz8Qm7p0eScEpUxZc0SQQVQUKhc+sp8DqbnIAIPDGhho3ZWYN+N6c2m/emM/XoDmbkFrNh5lAB/oUN4HQL8/cjMzWfT/nQ+WLyTH38/cMq+XZrU44EBLXj6m9/Zm5rFZyv3kF+o59xD25iyZImgAsgrKORgeg79W4VxU9dISwIVXMfIumRkO/ok3P/pGtf623o2Ib9QTxnEr0Y1fzpH1ePuPk25tEUoAf6OacI/G92DT5bvYtaG/RRai2njZZYIvOzDJTv556xNAHRv1oAhsY29HJFxx+FjJ6fbbHdxbTbuS+djZx+E2kEBdIysy+29ohnQumGJzwAi69fgyavaMGvD/nKL2ZgzsUTgZVsPZFA9wI/7+zRjeCd7LlBZXNspnMQjmTw0MIac/ELaPPMTAO/d2oXB7Ww8I1O5+GQi2J+Wxc3vL2dDUpqreebCLYdIy8pjxDv/47HBrejerEGZnvOlH+JZtSuFejWq8fbNnSlUZcx/1rAhKY2QoEDrnFTJiAgPO39nwdX8rUmvqdR8MhFs3JdOamYel0TVZUQXRyemxGTHnLyrdqWwdEdymSeCr9bsJT0rj/xCZW9qJrn5ysIth2nTuDZD29s3SF+VlJJFUspe0rPyef2mjoQElTyukjGe5JOJ4IQXhseWS7O91MxcClUJCQogJTOPjfvSid+fDsBDA2NsaGTDvE0HSTh8nI4lDCVijKf5dCIoaycmhj+Wk0ePZg24vnMEX6zcw+NfrQdwdTx68YdNHM7IwU8co4ca3zXzL5eyLimVp775nYc/X0tIUADVA/355/XtaR5WvFe5MZ5giaAMbdyXzkdLdyICqxJTuL5zBPvTHK1Lxg1vR05eIS/N2kR+QSHdousz5Y6upwx1bHxPbHgdvneOvHpiOGyA9UmplghMufHzdgBVUZ3gQBTHhCcnmhne0r0JRzNzAcecAYEBYknAAPCHOMecEe/e0oVv7ncMzf3p8t38mpDszbCMD7FPojKSmZtPUkqmazkrt4BLX11Abn4h1QMc+TbhsGNM+3YX1+a+vi28EqepeFo0DHG1Olq2w/Hhv2pXCjdNXs6DA1rwh7hI8guVz1bsZldyJjWrB/DssLbUtgfLpoxYIigjoz9ezZLtRwAICvAnJ7+A3PxCboyL4Lae0fgVmQLxLwNauDXxivE93ZvWJ9BfyCtw9DZ+Y/523pi/vVi5P8RF0KOMW7YZ3+VWIhCR3sBzQBPnPgKoqjbzXGiVS2pWLrHhtfnboFb8d00S85zzCbRpXNvVMik6tCYAjesEey1OU7H5+QnbXrqK/20/wpz4g0z9XyIA9WtW45mr25JbUMjj/13PyMnLAQj0F6be0c2mzzQXxN0awYfAw8BqoMBz4VRujUKC6N+6IXdMXelaF1CkJvD3wa3p36qhNRE0perVIpReLUJpfVEIvVuEElm/BgD/Xnhq7SCvQNmVnElvu9NoLoC7iSBNVX/0aCSVVFZuAT9t3E/K8TwahZw6j+0zV7flmo4nh43w8xOrzptzMrJb1CnLd/Zuyg/r9/PVfb1Izcyjxz9/9lJkpipxNxEsEJEJwNdAzomVqrrmzLtULqG1qnHkWO457zd74wEe/nwd4JgGsag7L21aJrEZc0JQoD8/PNgHAJE8L0djqgp3E0F358+4IusUGHC2nURkCPAvwB/4QFXHn7b9daC/c7EG0FBV67oZU5la8eTlFKgS89S5VXxyCwoB+Oq+XiVOMG+Mpz35zQYmzdtKXkEhd/dpxsiukTSoZR0VjfvcSgSq2r/0UqcSEX/gbWAQkASsFJEZqhpf5LgPFyn/F+CScz1PWfHzE/w4/ykDG9Wujr/zeUCfmFAWbztSVqEZU6LUzJM1gkMZjor6hNlbeH3uVlY+dTl1awSyaX8GGdl5NA2tSd0a1cjOLyAowJ9qAdaFyJzkbquhOsCzwGXOVb8A41Q17Sy7dQO2q2qC8xifAcOB+DOUH+U8R6WxLzWLxCK9QU/45K7uJZQ2pmzFNHT0PP7XyE70b92QDs/NASC/ULnv09XsTs5kX1p2sf0a1wli6d8HnNKk2fg2d78WfARkADc6X+nAlFL2CQf2FFlOcq4rRkSaAE2B+W7GUyEMf3sp/164A4Bgm1XMlDM/PyFx/FCGdwqndlAgieOHMqhtIwCWJxxlX1o27S6u7Sof6C9UC/Bjf1q2zYpmTuHuM4LmqnpDkeXnRWRtGcYxEvivqpbYNFVERgOjAaKiokoq4hXHsvO5qv1FjOnfwu7Jmgrhtp5NmBt/kMm3dqFrdH3q1azG8oRkEg4f5+buUbzx8zZem7uVN+Zv50BaFnVrVOPxwa1cU2geOZbD0u1HePWnLVQP9CM9K493bulC7MV1CK5mX3aqKncTQZaIXKqqS8DVwSyrlH32ApFFliOc60oyEhhzpgOp6mRgMkBcXFyF+ioTUa8G7S72/FDWxrijT0xYsUlyejRr4Gq2/PlKRyX9jZ+3ubZ3jqpLUkoWr8/dyvHc4t/F/vDuMm7r2YRxw2M9GLnxJncTwX3ANOezAgGOAreXss9KIEZEmuJIACOBm08vJCKtgXrAMjdjMcacp2svuZi3F+zg1Rs68NueVKav2M2f/9/JVuDNwmoypN1F3NQ1kg1703jgP78B8PGyXQRX8+eRQS2pHmA1g6rG3VZDa4GOIlLbuZzuxj75IvIAMBtH89GPVHWjiIwDVqnqDGfRkcBnqpXnpuXGfWl8uGQnOfnWydpULo8Nbs1jg1sDcDw3H4BLoupyQ+cIhsReRGiRW5xNGtTk6g4XEz32BwDe+yWBq2IbW8/4KuisiUBEblHV/ycij5y2HgBVfe1s+6vqLGDWaeueOW35uXOIt0L4ft1+vl6zl+gGNegcVc/b4RhzXu7o3ZRbezRxPR84k3f+2Jkdh48xcc5WKs23NXNOSqsR1HT+DPF0IJVNtQA/Fj52zt0rjKlQSksCAFe2b8yCzYfKIRrjLWdNBKr6nvPn8+UTjjGmItp6MAOAa99eCsCTV7VmZLcomxOhinCrH4GIvCoitUUkUER+FpHDInKLp4MzxlQMrS469abAy7M20+G5Odz4rrXxqArc7VB2hfMB8dVAItACeMxTQXnS3tSsU7rmnwtV5cMlO1luUwgaH9OvVUMSxw/ltRs7cu9lJ6chWZF4lFd/2szu5Myz7G0qOnebj54oNxT4UlXTTjwwrqxCa1WnUe2g0gsWcSgjhxdmxuPvJ3SMsL4Dxvdc39kxv/ITV7VxtSb698IdBPgJj1zRypuhmQvgbiKYKSKbcXQiu09EwoDig5hUIquevvyc9znRLf+la2OLjRNvjK+Z+ZdLHb2QZ2+h0JoTVWpu3RpS1bFALyBOVfOA4zgGkDPG+KjY8Drc27e5t8MwZaC0fgQDVHW+iFxfZF3RIl97KjBjjDHlo7RbQ31xjAh6TQnbFEsExhhT6ZXWj+BZ5887yieciuuT5buYsmSnt8Mwxpgy524/gpdFpG6R5Xoi8qLHoqqAFm09zOGMHIZ1vJjeLUJL38EYH1FQqLy1YDtPfL2ewkIlv6CQr9ckcc2bS+j60jz6vDqf1btSvB2mOQt3Ww1dqapPnlhQ1RQRuQp42jNhVUwR9WvwxiivzaZpTIU2fcUepq/YU+K2zQfS6dLExuWqqNztUOYvIq5hCUUkGLCZWIwxTL+nB83CarqWReCuS5uy5O/9+d/YAV6MzLjL3RrBp8DPInJieso7gGmeCckYU5n0bN6Aj/7UlUe+WMvHd3WnZjV/V+vCQ+mVuruRz3B3PoJXRGQdcKIX1guqOttzYRljKpPo0Jp8fX9vb4dhzpO7NQKATUC+qs4TkRoiEqKqGZ4KrKLIKyjk+3X72HM08/Q+FMYYN+XkFfJrQjIKdIqsS1CgzXJWkbiVCETkHhyTx9cHmgPhwLvAQM+FVjGs2ZXCI1+sA6B/qzAvR2NM5ZLvHHti3Mx417oHB8bwyKCW3grJlMDdh8VjgN5AOoCqbgMaeiqoiiSvwPGH/P5tcbx/W5yXozGmcilaie7n/CI1ZelOft500EsRmZK4e2soR1VzT9waEZEA8K1Z6+rWCHRrNidjzEmN6wQz5+HLaBZakwB/P6LH/kBGdj53TVvF00Pb0CcmjDkbD5CdX0DnqHoMbNPI2yH7JHcTwS8i8iQQLCKDgPuB7z0XljGmqmjZ6OSkNn1iQlm87QgAL/6wCcejx5MeGdSSkd0iaRhybkPEmwvj7lfcvwOHgQ3AvTgmpC+1M5mIDBGRLSKyXUTGnqHMjSISLyIbReQ/7gZujKl8PrmrO3MfvuyUdS9cG+t6/9rcrfywfn95h+XzSq0RiIg/sFFVWwPvu3tg535vA4OAJGCliMxQ1fgiZWKAJ4Dezt7KPvHcwRhfFtMohMTxQ09Z16R+DdbsTmHSvG0cOZZDdl6Bq2VRVm4BeYWF1KoWgJ+ftdzzhFITgaoWOL/VR6nq7nM4djdgu6omAIjIZzjmMIgvUuYe4G1VTXGe69A5HN/jsvMKOGgdYozxuMtahtH24tpMmreNtxfs4O0FO4qVub5zOK/d2Kn8g/MB7j4jqAdsFJEVOCalAUBVh51ln3Cg6MAjSUD308q0BBCRpYA/8Jyq/uRmTB5324crWJF4FIBq9qDYGI+qHRRY4no/gUKFr9fspVNkXf7QJZLgatYPoSy5mwj+4cHzxwD9gAhgkYi0V9XUooVEZDSOfgxERZXfFJHJx3PoGFmXMf2a0z7c5ig2xpOqBfiROH4ohYVKTn4hv2w9RIuGIbRoWMs1P/Iz323kvV8SeHxIK4Z3CvdyxFXHWb/mikiQiPwV+APQGliqqr+ceJVy7L1AZJHlCOe6opKAGaqap6o7ga04EsMpVHWyqsapalxYWPl26oqoF8wV7S6ye5PGlBM/PyG4mj9DYhvTomEtAF6+rj0DWzseIe5NzeKhz9Yy+uNVbNyXBkBufiGH0rNJz87zWtyVWWk1gmlAHrAYuBJoCzzk5rFXAjEi0hRHAhgJ3HxamW+BUcAUEQnFcasowc3jG2N8xM3do7i5exQTZ2/hrQXbAZgTf5BdyZk0C6vJj78fcJV96bpYBrVpRMPa1gTVXaXd+G6rqreo6nvACKCPuwdW1XzgAWA2jsbCX6jqRhEZJyInni3MBpJFJB5YADymqsnnfBXGGJ/w6OBWrHzqcr65vxcAWw5m8OPvB7ioyIf+U9/8zkOfrfVShJVTaTUCVz1LVfPPddA1VZ2Fo89B0XXPFHmvwCPOlzHGlCospDqhtaoBMKhtIx4aGENseB0OpGXT458/06pRCJm5+V6OsnIpLRF0FJF053vB0bM43fleVbW2R6MzxpgSiEixvggX1QkicfxQbp+ygpTjuV6KrHI6660hVfVX1drOV4iqBhR5b0nAGFPhbD90jHVJaUxfsdtqBm6yxvElWJ+UytA3FrPnaJa3QzHGnKOkFMf/2ye+3kDbZ2bztvPhsjkzSwQlWLcnlY370rk0JpQRXSK8HY4x5hz88/r2DGp7chTTCbO30P7Z2cxYt8+LUVVs5zJDmc+ZMKIDDWpV93YYxphzMKpbFKO6RbFo62He+Hkbq3alkJGTz4PTf+Pv/13P2Ctb4ydQs3oAwzpebMPLY4nAGFNFXdYyjMtaOjqgnuiZnJVXwLMzNrrKHM/JZ/gl4Wcc3sJXWCo0xlR5X93Xkwf6twBgwaP9eGigYwCDf3y3kbumrvRmaBWCJQJjTJXXpUl9Hh3cisTxQ2kaWpOR3Ryj31xcJ4i0LBuWwhKBMcbnNK4TTOL4oexLy2brwWN0fWkeK3Ye9XZYXmOJwBjjswL9HaMlHM7IYevBDC9H4z2WCIwxPmvbS1ex4qmB3g7D6ywRlCAl0+4ZGuMrCgsdP5/+9nee/37j2QtXUZYIzsDfT6geaLMgGVPVFR1L85Nlu7wXiBdZIjiDWQ/2oVZ162ZhTFXXqHYQCx/tx5/7Nvd2KF5jieAMWl0U4u0QjDHlJDq0Jr7cwdiHL90YYwxYIjDGGJ9nicAYY3ycJQJjjPFxlgiMMcbHWSIwxhgf59FEICJDRGSLiGwXkbElbL9dRA6LyFrn625PxmOMMaY4jyUCEfEH3gauBNoCo0SkbQlFP1fVTs7XB56Kxxhjzuan3w+QX6h0eWEu//l1N3kFhd4Oqdx4skbQDdiuqgmqmgt8Bgz34PnKTFCg3TEzxtdE1KsBQPLxXJ78ZgOdx831mbkKPDmGQjiwp8hyEtC9hHI3iMhlwFbgYVXdc3oBERkNjAaIioq6oKCeubotbRrXPuP2B/q3YHiniy/oHMaYymfand2Ak9NaZuTkk5qZS53gqj+Npbe/+n4PRKtqB2AuMK2kQqo6WVXjVDUuLCzsgk5456VN6dm8wRm3Pzq4FTGNbHgJY3xV4vihvHZjRwCSUrLI94FbRJ5MBHuByCLLEc51LqqarKo5zsUPgC4ejMcYY9yyaOthAP74wa/EPjeb+ZsPFksIufmF5OQXeCO8MufJW0MrgRgRaYojAYwEbi5aQEQaq+p+5+IwYJMH4zHGGLeM6BLJt2v3AZCdV8idU1edsewTV7bm3ko+cqnHagSqmg88AMzG8QH/hapuFJFxIjLMWexBEdkoIuuAB4HbPRWPMca469KYUBLHD+WePk3pGFGnxDLV/P2oUc2fxOTMco6u7Hl0wH1VnQXMOm3dM0XePwE84ckYjDHmfD019GSL9x2Hj3EwLZvM3AK6N6tPSFAgXV+a58Xoyo7NvGKMMW5oHlaL5mG1vB2GR3i71ZAxxhgvs0RgjDE+zhKBMcb4OEsExhjj4ywRGGOMj7NEYIwx5+lwRg7TV+ym74QFfLU6id2VtE+BJQJjjLlAu5Iz+duX63j6u9+9Hcp5sX4Exhhznr66rydBgf7c+uEKwusGk5NXOcceshqBMcacpy5N6tPu4jqs+ccgth3K4NedR/lgcQLHc/K9Hdo5sURgjDFlIDvPMTrpiz9sYvG2I16O5txYIjDGmDLwwW1xPDKoJQCFql6O5txYIjDGmDJwedtGDG53kbfDOC+WCIwxxsdZIjDGGB9nicAYY8rI8VxHa6H7P13DuO/jvRyN+ywRGGNMGalZ7WTXrBnr9hG/L5158QfZkJTmxahKJ1rJnm7HxcXpqlVnnj/UGGO8rcWTs8gvPPnZGuAnbHhuMMHV/L0Wk4isVtW4krZZjcAYY8pY68YhAIzsGsnlbRqRX6jkFRZ6OaozsyEmjDGmjM38Sx/X+w8WJzBv00EvRlM6j9YIRGSIiGwRke0iMvYs5W4QERWREqstxhhjPMdjNQIR8QfeBgYBScBKEZmhqvGnlQsBHgJ+9VQsxhjjLXtTswDo8NwcQoIC6NsyjPE3dKBW9YpzQ8aTNYJuwHZVTVDVXOAzYHgJ5V4AXgGyPRiLMcZ4xe97T7YYysjOZ+b6/Ww7mOHFiIrzZEoKB/YUWU4CuhctICKdgUhV/UFEHjvTgURkNDAaICoqygOhGmOMZ0y5oxsrE49ySWRdlu1I5r5P13A4I4f8gkIC/CtGex2vRSEifsBrwN9KK6uqk1U1TlXjwsLCPB+cMcaUkVrVA+jfqiF1a1Rjx+FjAIz+ZDX3fbrGy5Gd5MlEsBeILLIc4Vx3QggQCywUkUSgBzDDHhgbY6qqIbEnB6WbG3+QV3/azNHjuV6MyMGTiWAlECMiTUWkGjASmHFio6qmqWqoqkarajSwHBimqtZbzBhTJbVoGELi+KGu5X8v3MEvWw95MSIHjyUCVc0HHgBmA5uAL1R1o4iME5FhnjqvMcZUdD8+1Id/jewEQEXoZ+bR9kuqOguYddq6Z85Qtp8nYzHGmIqiTeParnGJMvMKKChU/P3Ea/FUjEfWxhjjY9Kz8wD4x7e/c/WbS7waiyUCY4zxgujQmq73e45mejESSwTGGOMVtaoHkDh+KHdd2tTbodigc8YY402frdjN8dwCosf+AMD1ncOZMKJjuT4zqBKJIC8vj6SkJLKzbZQK411BQUFEREQQGBjo7VBMJTG0Q2O+WJXkWv56zV7GDmlNw9pB5RZDlZiYZufOnYSEhNCgQQNEvPfk3fg2VSU5OZmMjAyaNvV+dd9ULtl5Bby/KIH/m7uVW3s04ebuUbRpXLvMjl/lJ6bJzs62JGC8TkRo0KCB1UzNeQkK9GfN7hQAPlm+i7fmby+3c1eJRABYEjAVgv0dmgvxr1GX8PTQNrRsVIuCwvK7W1NlEoExxlR2tYMCubtPM4Ty/UJhiaAM7Nmzh6ZNm3L06FEAUlJSaNq0KYmJiQBs27aNq6++mubNm9OlSxf69+/PokWLAJg6dSphYWF06tSJdu3aMWLECDIzT7YpnjhxIq1bt6ZTp0507dqVjz/+GIB+/fpx+rOS87Vq1SoefPBBAHJycrj88svp1KkTn3/+OXfffTfx8fGlHOHsJk2a5IobID8/n7CwMMaOPXXSun79+tGqVSs6duxI79692bJlywWdF2DatGnExMQQExPDtGnTzljuzTffpHXr1rRr147HH38cgNzcXO644w7at29Px44dWbhwoav85ZdfTkpKygXHZ0yFoKqV6tWlSxc9XXx8fLF15e2VV17Re+65R1VVR48erS+//LKqqmZlZWlMTIx+9913rrIbNmzQKVOmqKrqlClTdMyYMa5to0aN0o8++khVVd955x294oorNC0tTVVV09LSdOrUqaqq2rdvX125cmWZX8eyZct04MCB571/fn7+Kct5eXnavn17zcvLc62bNWuW9urVS5s1a6aFhYWu9UWv6b333tNrrrnmvONQVU1OTtamTZtqcnKyHj16VJs2bapHjx4tVm7+/Pk6cOBAzc7OVlXVgwcPqqrqW2+9pbfffrtrXefOnbWgoEBVVadOnaovvvhiieetCH+PpnK74rVf9N6PV5XpMYFVeobP1SrRfLSo57/fSPy+9DI9ZtuLa/PsNe3OWubhhx+mS5cuTJo0iSVLlvDWW28B8Omnn9KzZ0+GDTs5zl5sbCyxsbHFjpGfn8/x48epV68eAC+//DILFy6kdm1Hy4HatWvzpz/9qdh+9913HytXriQrK4sRI0bw/PPPAzB27FhmzJhBQEAAV1xxBRMnTuTLL7/k+eefx9/fnzp16rBo0SIWLlzIxIkT+eijj7jllls4fPgwnTp14quvvuKuu+5i4sSJxMXFMWfOHJ599llycnJo3rw5U6ZMoVatWkRHR3PTTTcxd+5cHn/8cUaOHOmKbf78+XTu3JmAgJN/atOnT+ehhx7inXfeYdmyZfTq1avYNV122WVMmjTprP/mpZk9ezaDBg2ifv36AAwaNIiffvqJUaNGnVLunXfeYezYsVSvXh2Ahg0bAhAfH8+AAQNc6+rWrcuqVavo1q0bw4YNo0+fPjz11FMXFKMxJdlyMIMtBzPo9c+f+fzenkTWr+HR81W5ROAtgYGBTJgwgSFDhjBnzhxXO/KNGzfSuXPns+77+eefs2TJEvbv30/Lli255pprSE9PJyMjg2bNmpV67pdeeon69etTUFDAwIEDWb9+PeHh4XzzzTds3rwZESE1NRWAcePGMXv2bMLDw13rTmjYsCEffPABEydOZObMmadsO3LkCC+++CLz5s2jZs2avPLKK7z22ms884xjDMEGDRqwZk3xiTaWLl1Kly5dXMvZ2dnMmzeP9957j9TUVKZPn15iIvj+++9p3759sfUTJkzg008/Lbb+sssu44033jhl3d69e4mMPDklRkREBHv37j19V7Zu3crixYt56qmnCAoKYuLEiXTt2pWOHTsyY8YMRo0axZ49e1i9ejV79uyhW7du1KtXj5ycHJKTk2nQoEGxYxpTFvalZbMnJdMSwbkq7Zu7J/344480btyY33//nUGDBpVY5rrrrmPbtm20bNmSr7/+GoCbbrqJt956C1VlzJgxTJgwgfvvv9/t837xxRdMnjyZ/Px89u/fT3x8PG3btiUoKIi77rqLq6++mquvvhqA3r17c/vtt3PjjTdy/fXXu32O5cuXEx8fT+/evQHH/fOePXu6tt90000l7rd//37atGnjWp45cyb9+/cnODiYG264gRdeeIFJkybh7+8PwB//+EeCg4OJjo7mzTffLHa8xx57jMceO+OspuclPz+fo0ePsnz5clauXMmNN95IQkICd955J5s2bSIuLo4mTZrQq1cvV5zgSJz79u2zRGDKXOL4oSxPSGbk5OXlcj57WFxG1q5dy9y5c1m+fDmvv/46+/fvB6Bdu3anfFP+5ptvmDp1quvBclEiwjXXXMOiRYuoXbs2tWrVIiEh4azn3blzJxMnTuTnn39m/fr1DB06lOzsbAICAlixYgUjRoxg5syZDBkyBIB3332XF198kT179tClSxeSk5Pduj5VZdCgQaxdu5a1a9cSHx/Phx9+6Npes2bNEvcLDg4+pV399OnTmTdvHtHR0a7zz58/37X9008/Ze3atXz77benfJs/YcKECXTq1KnY68TD7qLCw8PZs+fktNlJSUmEh4cXKxcREcH111+PiNCtWzf8/Pw4cuQIAQEBvP7666xdu5bvvvuO1NRUWrZs6dovOzub4ODgUv7ljKn4LBGUAVXlvvvuY9KkSURFRfHYY4/x6KOPAnDzzTezdOlSZsxwTc52Squg0y1ZsoTmzZsD8MQTTzBmzBjS0x3PPI4dO3ZK6xuA9PR0atasSZ06dTh48CA//vijq2xaWhpXXXUVr7/+OuvWrQNgx44ddO/enXHjxhEWFnbKB+XZ9OjRg6VLl7J9u6OTy/Hjx9m6dWup+7Vp08a1T3p6OosXL2b37t0kJiaSmJjI22+/zfTp092KARw1ghPJqOjr9NtCAIMHD2bOnDmkpKSQkpLCnDlzGDx4cLFy1157LQsWLAAct4lyc3MJDQ0lMzOT48ePAzB37lwCAgJo27Yt4PidHzhwgOjoaLdjN+Zc5Bc4+hG8NX87//l1N9sOZpCa6ZlpLavcrSFveP/994mKinLdDrr//vuZMmUKv/zyC3379mXmzJk88sgj/PWvf6VRo0aEhITw9NNPu/Y/8YygsLCQiIgIpk6dCjgeAh87doyuXbsSGBhIYGAgf/vb3045d8eOHbnkkkto3bo1kZGRrls3GRkZDB8+nOzsbFSV1157DXB8kG7btg1VZeDAgXTs2JFffvml1GsMCwtj6tSpjBo1ipycHABefPHFU74hl+TKK6/k1ltvBRy1oQEDBrgeygIMHz6cxx9/3HXMslS/fn3+8Y9/0LVrVwCeeeYZ14Pju+++mz//+c/ExcVx5513cueddxIbG0u1atWYNm0aIsKhQ4cYPHgwfn5+hIeH88knn7iOvXr1anr06HHKQ3BjytKeFMcXxv/tSOZ/Oxw19xevjeWWHk3K/FxVYqyhTZs2nXIf2lQs1113Ha+++ioxMTHeDqXMPPTQQwwbNoyBAwcW22Z/j6YsqCqvz9tGelYeX6zaw6sjOtA+vA5NGpR8G7Y0ZxtryL7OGI8bP348+/fvr1KJIDY2tsQkYExZEREeGeSocT83zLONYCwRGI9r1aoVrVq18nYYZeqee+7xdgjGlBmPPiwWkSEiskVEtovI2BK2/1lENojIWhFZIiJtz/dcle0Wl6ma7O/QVEYeSwQi4g+8DVwJtAVGlfBB/x9Vba+qnYBXgdfO51xBQUEkJyfbf0LjVeqcjyAoqPwmFDGmLHjy1lA3YLuqJgCIyGfAcMA1gpmqFh0LoiZwXp/kERERJCUlcfjw4QsI15gLd2KGMmMqE08mgnCgaCP1JKD76YVEZAzwCFANGFDSgURkNDAaICoqqtj2wMBAmxHKGGPOk9c7lKnq26raHPg78PQZykxW1ThVjQsLCyvfAI0xporzZCLYCxQdIyDCue5MPgOu9WA8xhhjSuDJRLASiBGRpiJSDRgJzChaQESKNiwfCmzzYDzGGGNK4LFnBKqaLyIPALMBf+AjVd0oIuNwTJAwA3hARC4H8oAUoPhg+6dZvXr1ERHZdZ5hhQJHznPfysqu2TfYNfuGC7nmM45NUemGmLgQIrLqTF2sqyq7Zt9g1+wbPHXNXn9YbIwxxrssERhjjI/ztUQw2dsBeIFds2+wa/YNHrlmn3pGYIwxpjhfqxEYY4w5jSUCY4zxcVUyEbgx/HV1Efncuf1XEYn2Qphlyo1rfkRE4kVkvYj8LCJlP99dOSvtmouUu0FEVEQqfVNDd65ZRG50/q43ish/yjvGsubG33aUiCwQkd+cf99XeSPOsiIiH4nIIRH5/QzbRUTecP57rBeRzhd8UlWtUi8cndd2AM1wDGS3Dmh7Wpn7gXed70cCn3s77nK45v5ADef7+3zhmp3lQoBFwHIgzttxl8PvOQb4DajnXG7o7bjL4ZonA/c537cFEr0d9wVe82VAZ+D3M2y/CvgREKAH8OuFnrMq1ghcw1+rai6OMYyGn1ZmODDN+f6/wEARkXKMsayVes2qukBVM52Ly3GM/VSZufN7BngBeAXILs/gPMSda74HeFtVUwBU9VA5x1jW3LlmBWo739cB9pVjfGVOVRcBR89SZDjwsTosB+qKSOMLOWdVTAQlDX8dfqYyqpoPpAENyiU6z3Dnmou6C8c3isqs1Gt2VpkjVfWH8gzMg9z5PbcEWorIUhFZLiJDyi06z3Dnmp8DbhGRJGAW8JfyCc1rzvX/e6lszmIfIyK3AHFAX2/H4kki4odjxrvbvRxKeQvAcXuoH45a3yIRaa+qqd4MysNGAVNV9f9EpCfwiYjEqmqhtwOrLKpijcCd4a9dZUQkAEd1MrlcovMMt4b8dg7w9xQwTFVzyik2TyntmkOAWGChiCTiuJc6o5I/MHbn95wEzFDVPFXdCWzFkRgqK3eu+S7gCwBVXQYE4Ricrao61yH+S1UVE0Gpw187l0+MdDoCmK/OpzCVlDtDfl8CvIcjCVT2+8ZQyjWrapqqhqpqtKpG43guMkxVV3kn3DLhzt/2tzhqA4hIKI5bRQnlGGNZc+eadwMDAUSkDY5EUJXnrZ0B3OZsPdQDSFPV/RdywCp3a0jdG/76QxzVx+04HsqM9F7EF87Na54A1AK+dD4X362qw7wW9AVy85qrFDeveTZwhYjEAwXAY6paaWu7bl7z34D3ReRhHA+Ob6/MX+xEZDqOZB7qfO7xLBAIoKrv4ngOchWwHcgE7rjgc1bify9jjDFloCreGjLGGHMOLBEYY4yPs0RgjDE+zhKBMcb4OEsExhjj4ywRGFMCESkQkbUi8ruIfC8idcv4+InOdv6IyLGyPLYx58oSgTEly1LVTqoai6OvyRhvB2SMp1giMKZ0y3AO6iUizUXkJxFZLSKLRaS1c30jEflGRNY5X72c6791lt0oIqO9eA3GnFGV61lsTFkSEX8cwxd86Fw1Gfizqm4Tke7Av4EBwBvAL6p6nXOfWs7yd6rqUREJBlaKyFeVuaevqZosERhTsmARWYujJrAJmCsitYBenBymA6C68+cA4DYAVS3AMbQ5wIMicp3zfSSOAeAsEZgKxRKBMSXLUtVOIlIDxzg3Y4CpQKqqdnLnACLSD7gc6KmqmSKyEMeAaMZUKPaMwJizcM7q9iCOgc0ygZ0i8gdwzR3b0Vn0ZxxTgCIi/iJSB8fw5inOJNAax1DYxlQ4lgiMKYWq/gasxzEByh+Bu0RkHbCRk9MmPgT0F5ENwGocc+f+BASIyCZgPI6hsI2pcGz0UWOM8XFWIzDGGB9nicAYY3ycJQJjjPFxlgiMMcbHWSIwxhgfZ4nAGGN8nCUCY4zxcf8fAPQV16QoKmEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot precision_recall\n",
    "\n",
    "metrics.plot_precision_recall_curve(xgb, test_verb_stack, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fee9f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fd592a2bdf0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot ROC curve\n",
    "\n",
    "metrics.plot_roc_curve(xgb, test_verb_stack, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cae0c",
   "metadata": {},
   "source": [
    "## Stochasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29845f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running latest pipeline with different seeds\n",
    "\n",
    "clean_training_data = preprocess_training_data(training_data)\n",
    "X = [x[0] for x in clean_training_data]\n",
    "y = [x[1] for x in clean_training_data]\n",
    "\n",
    "random_seeds = [4, 22, 235, 42, 55]\n",
    "different_splits = {}\n",
    "for seed in random_seeds:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)    \n",
    "    different_splits[f'seed_{seed}'] = {}\n",
    "    different_splits[f'seed_{seed}']['X_train'] = X_train\n",
    "    different_splits[f'seed_{seed}']['X_test'] = X_test\n",
    "    different_splits[f'seed_{seed}']['y_train'] = y_train\n",
    "    different_splits[f'seed_{seed}']['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e4ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_pipeline(X_train, y_train, X_test, y_test):\n",
    "    #vectorise \n",
    "    X_train_vec = sent_class.fit_transform(X_train)\n",
    "    X_test_vec = sent_class.fit_transform(X_test)\n",
    "    #stack \n",
    "    train_verb_stack = np.hstack((X_train_vec, count_verbs(X_train)))\n",
    "    test_verb_stack = np.hstack((X_test_vec, count_verbs(X_test)))\n",
    "    #classify\n",
    "    xgb = XGBClassifier(max_depth= 7, min_child_weight= 1)\n",
    "    xgb.fit(train_verb_stack, y_train)\n",
    "    predict = xgb.predict(test_verb_stack)\n",
    "    print(classification_report(y_test, predict))\n",
    "    \n",
    "    return X_train_vec, X_test_vec, train_verb_stack, test_verb_stack, xgb, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ca269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained on seed_4\n",
      "2021-08-06 09:51:50,229 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:51:50,230 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:51:50,231 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-06 09:51:50,232 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:51:50,548 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 6188 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-06 09:51:50,548 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-06 09:51:50,549 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-06 09:51:57,232 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 155\n",
      "Took 51.668967962265015 seconds\n",
      "2021-08-06 09:52:42,227 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:52:42,227 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:52:42,227 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-06 09:52:42,229 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:52:42,547 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 3049 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-06 09:52:42,547 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-06 09:52:42,547 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-06 09:52:49,509 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 77\n",
      "Took 38.38478207588196 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/india.kerlenesta/opt/anaconda3/envs/skills-taxonomy-v2/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:53:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91      2384\n",
      "           1       0.77      0.48      0.59       665\n",
      "\n",
      "    accuracy                           0.85      3049\n",
      "   macro avg       0.82      0.72      0.75      3049\n",
      "weighted avg       0.85      0.85      0.84      3049\n",
      "\n",
      "------------\n",
      "model trained on seed_22\n",
      "2021-08-06 09:53:36,572 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:53:36,573 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:53:36,573 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-06 09:53:36,574 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-06 09:53:36,874 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 6188 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-06 09:53:36,874 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-06 09:53:36,875 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    }
   ],
   "source": [
    "for key, value in different_splits.items():\n",
    "    print(f'model trained on {key}')\n",
    "    X_train_vec, X_test_vec, train_verb_stack, test_verb_stack, xgb, predict = current_pipeline(different_splits[key]['X_train'], different_splits[key]['y_train'], \n",
    "                     different_splits[key]['X_test'], different_splits[key]['y_test'])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "81fa4762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-05 17:09:42,728 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 17:09:42,729 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 17:09:42,730 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 17:09:42,731 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 17:09:43,032 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 8313 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 17:09:43,032 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 17:09:43,032 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 17:09:49,726 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 208\n",
      "Took 73.8318247795105 seconds\n",
      "2021-08-05 17:10:56,873 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 17:10:56,874 - sentence_transformers.SentenceTransformer - INFO - Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 17:10:56,874 - sentence_transformers.SentenceTransformer - INFO - Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-08-05 17:10:56,875 - sentence_transformers.SentenceTransformer - INFO - Load SentenceTransformer from folder: /Users/india.kerlenesta/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-08-05 17:10:57,207 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Getting embeddings for 924 texts ...\n",
      ".. with multiprocessing\n",
      "2021-08-05 17:10:57,207 - sentence_transformers.SentenceTransformer - INFO - CUDA is not available. Start 4 CPU worker\n",
      "2021-08-05 17:10:57,207 - sentence_transformers.SentenceTransformer - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2021-08-05 17:11:04,333 - sentence_transformers.SentenceTransformer - INFO - Chunk data into packages of size 24\n",
      "Took 16.369921922683716 seconds\n",
      "[17:11:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       731\n",
      "           1       0.80      0.60      0.68       193\n",
      "\n",
      "    accuracy                           0.89       924\n",
      "   macro avg       0.85      0.78      0.81       924\n",
      "weighted avg       0.88      0.89      0.88       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#best random seed + experiment based on maximising precision \n",
    "#print false positive sentences\n",
    "\n",
    "X_train_vec, X_test_vec, train_verb_stack, test_verb_stack, xgb, predict = current_pipeline(different_splits['seed_22']['X_train'], different_splits['seed_22']['y_train'],\n",
    "                 different_splits['seed_22']['X_test'], different_splits['seed_22']['y_test'])\n",
    "\n",
    "predicted_probabilities = xgb.predict_proba(test_verb_stack)\n",
    "for sentence, probability, test_label, predicted_label in zip(different_splits['seed_22']['X_test'], predicted_probabilities, different_splits['seed_22']['y_test'], predict):\n",
    "    if test_label == 0 and predicted_label == 1:\n",
    "        print(sentence, '------',  probability[1])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "skills-taxonomy-v2",
   "language": "python",
   "name": "skills-taxonomy-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
